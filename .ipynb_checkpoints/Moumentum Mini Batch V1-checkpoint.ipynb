{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e14a66-e0bb-4776-9c66-338208e462bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337b4830-56ca-4f92-ad76-b5ae00e6c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBGD_LR(x, y, alpha=0.05, batch_size=100, epochs=10, gradient_norm=0.001, loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. Basic preprocessing and data conversion\n",
    "    m = x.shape[0]\n",
    "    x = np.array(x)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "    # 2. Shuffle data at the start to ensure batches are representative and prevent bias\n",
    "    all_data = np.concatenate((x, y), axis=1)\n",
    "    np.random.shuffle(all_data)\n",
    "    x = all_data[:, :-1]\n",
    "    y = all_data[:, -1].reshape(-1, 1)\n",
    "\n",
    "    # 3. Add intercept (Bias) term\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    \n",
    "    # 4. Initialize parameters\n",
    "    w = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    loss = []         # Stores loss for every single batch update\n",
    "    thetas = []       # Stores weight history\n",
    "    total_loss = []   # Stores the accumulated loss per epoch\n",
    "\n",
    "    for i in range(epochs):\n",
    "        start = 0\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        # 5. Mini-batch loop: Iterate over the dataset in small chunks\n",
    "        while start < m:\n",
    "            # Determine the end index of the current batch\n",
    "            end = start + batch_size if (start + batch_size) < m else m\n",
    "            x_temp = x[start:end, :]\n",
    "            y_temp = y[start:end, :]\n",
    "            m_temp = x_temp.shape[0]\n",
    "\n",
    "            # 6. Forward pass on the mini-batch\n",
    "            hx = x_temp @ w\n",
    "            e = hx - y_temp\n",
    "            \n",
    "            # 7. Compute Batch Loss (Cost)\n",
    "            j = (e.T @ e) / (2 * m_temp)\n",
    "            loss.append(j)\n",
    "            \n",
    "            # 8. Compute Gradient based ONLY on the current mini-batch\n",
    "            G = (x_temp.T @ e) / m_temp\n",
    "            thetas.append(w.copy())\n",
    "            \n",
    "            # 9. Update Weights (Standard GD rule applied per batch)\n",
    "            w = w - alpha * G\n",
    "            \n",
    "            # Move to the next batch\n",
    "            start = end\n",
    "            \n",
    "            # Accumulate loss to monitor epoch-level performance\n",
    "            total_epoch_loss += np.absolute(j)\n",
    "            \n",
    "        total_loss.append(total_epoch_loss)\n",
    "\n",
    "        # 10. Convergence Check: Gradient Norm (from the last batch of the epoch)\n",
    "        if np.linalg.norm(G) <= gradient_norm:\n",
    "            print(f'Gradient Norm Condition - Stopped at epoch {i}')\n",
    "            break\n",
    "            \n",
    "        # 11. Convergence Check: Stability of total epoch loss\n",
    "        if i > 2 and np.absolute(total_loss[i-1] - total_loss[i]) < loss_condition:\n",
    "            print(f'Loss Condition - Stopped at epoch {i}')\n",
    "            break\n",
    "\n",
    "    # 12. Evaluation: Manual R2 Score calculation across the entire dataset\n",
    "    y_mean = np.mean(y)\n",
    "    ss_res, ss_tot = 0, 0\n",
    "    for s in range(0, m, batch_size):\n",
    "        e = min(s + batch_size, m)\n",
    "        y_p = x[s:e, :] @ w\n",
    "        ss_res += np.sum((y[s:e] - y_p)**2)\n",
    "        ss_tot += np.sum((y[s:e] - y_mean)**2)\n",
    "    \n",
    "    print(f'Final R2 Score = {1 - (ss_res / ss_tot)}') \n",
    "    print('Training Finished.')\n",
    "    \n",
    "    return np.array(w), np.array(loss), np.array(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a111af-0352-4300-8da8-3e632a653afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBGD_LR(x,y,  alpha=0.05,batch_size=100 ,epochs=10 ,  gradient_norm=0.001  ,  loss_condition=0.001):\n",
    "    m=x.shape[0]\n",
    "    x=np.array(x)\n",
    "    y=np.array(y).reshape(-1,1)\n",
    "    all_data=np.concatenate((x,y),axis=1)\n",
    "    np.random.shuffle(all_data)\n",
    "    x=all_data[:,:-1]\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    y=all_data[:,-1].reshape(-1,1)\n",
    "    m=x.shape[0]\n",
    "    loss=[]\n",
    "    w=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    thetas=[] # return tensor \n",
    "    total_loss=[]\n",
    "    for i in range (epochs) :\n",
    "        start=0 \n",
    "        end= 0\n",
    "        total_epoch_loss=0\n",
    "        while(start<m):\n",
    "            end=start+batch_size if (start+batch_size)<m else m\n",
    "            x_temp=x[start:end,:]\n",
    "            y_temp=y[start:end,:]\n",
    "            m_temp=x_temp.shape[0]\n",
    "            hx=x_temp@w\n",
    "            e=hx-y_temp\n",
    "            j=e.T@e/(2*m_temp)\n",
    "            loss.append(j)\n",
    "            G=(x_temp.T@e)/m_temp\n",
    "            thetas.append(w.copy())\n",
    "            start=end\n",
    "            w=w-alpha*G\n",
    "            total_epoch_loss+=np.absolute(j)\n",
    "        total_loss.append(total_epoch_loss)\n",
    "        if np.linalg.norm(G)<=(gradient_norm):\n",
    "             print (f''' Gradient Norm condation\n",
    "             stop in epoch number {i} \n",
    "             gredient norm = {np.linalg.norm(G)}''')\n",
    "             break\n",
    "        if i>2 and np.absolute(total_loss[i-1]-total_loss[i])<loss_condition:\n",
    "            print (f'''loss condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={total_loss[i-1]-total_loss[i]}\n",
    "            ''')\n",
    "            break\n",
    "    y_mean = np.mean(y)\n",
    "    ss_res, ss_tot = 0, 0\n",
    "    for s in range(0, m, batch_size):\n",
    "        e = min(s + batch_size, m)\n",
    "        y_p = x[s:e, :] @ w\n",
    "        ss_res += np.sum((y[s:e] - y_p)**2)\n",
    "        ss_tot += np.sum((y[s:e] - y_mean)**2)\n",
    "    \n",
    "    print(f'Final R2 Score = {1 - (ss_res / ss_tot)}') \n",
    "    print('you finshed your itirations ')\n",
    "    \n",
    "    return np.array (w) ,np.array(loss),np.array(thetas)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b9600-232d-4fd5-a9a4-f5b326d352a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
