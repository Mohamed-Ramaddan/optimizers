{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d76c280-ec6a-4306-ada0-db6d2393039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503df97c-ac5a-45d1-aaa6-26a00604596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(r'E:\\Courses\\Advanced AI\\optimization\\Lab 2\\MultiVarLR.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e6abe2-8ad2-4c3f-87f3-889443d9f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,:-1]\n",
    "\n",
    "y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aabaf6b6-9787-44f1-bca7-8580aee64452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0   152.0\n",
       "1   185.0\n",
       "2   180.0\n",
       "3   196.0\n",
       "4   142.0\n",
       "5   101.0\n",
       "6   149.0\n",
       "7   115.0\n",
       "8   175.0\n",
       "9   164.0\n",
       "10  141.0\n",
       "11  141.0\n",
       "12  184.0\n",
       "13  152.0\n",
       "14  148.0\n",
       "15  192.0\n",
       "16  147.0\n",
       "17  183.0\n",
       "18  177.0\n",
       "19  159.0\n",
       "20  177.0\n",
       "21  175.0\n",
       "22  175.0\n",
       "23  149.0\n",
       "24  192.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs=pd.DataFrame(x)\n",
    "y2=pd.DataFrame(y)\n",
    "xs\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4172b93b-98f8-4189-b0fb-33af84d7cc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=x.shape[0]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4b2a7-8db7-4f78-a3e0-6f09d6623faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91d14c4d-576f-4e8b-a2ca-0228de269974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stander(col):\n",
    "    return (col - col.mean()) / (col.std() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c57ada8-8931-46cb-9eb5-d744a5753b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xs = (xs - xs.mean()) / (xs.std() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7437e-918a-4769-932d-75c795d5a828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeefec7-8fb1-49c7-9993-3cf1ca457803",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86a413-ac79-4d98-b563-d9b86b99b3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1612ab8-dcab-4cc4-834b-873594f24b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD_LR(x,y,alpha=0.05,itirations=1000,gradient_norm=0.001,loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    m=x.shape[0]\n",
    "    loss=[]\n",
    "    x=np.array(x)\n",
    "    y=np.array(y).reshape(-1,1)\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    w=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    thetas=[] # return tensor \n",
    "    for i in range (itirations):\n",
    "        hx=x@w\n",
    "        e=hx-y\n",
    "        j=e.T@e/(2*m)\n",
    "        loss.append(j)\n",
    "        G=(x.T@e)/m\n",
    "        thetas.append(w.copy())\n",
    "        if np.linalg.norm(G)<=(gradient_norm):\n",
    "            print (f''' Gradient Norm condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}''')\n",
    "            break\n",
    "        if i>2 and np.absolute(loss[i-1]-loss[i])<loss_condition:\n",
    "            print (f'''loss condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={loss[i]-loss[i-1]}\n",
    "            ''')\n",
    "            break\n",
    "        \n",
    "        w=w-alpha*G\n",
    "    hx=x@w\n",
    "    print('you finshed your itirations ')\n",
    "    print (f'R2 score ={r2_score(y,hx)}')\n",
    "    return np.array (w) ,hx,np.array(loss),np.array(thetas)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1388e081-bd7b-4c61-8e35-9d5eb66a7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD_LR(x, y, alpha=0.05, itirations=1000, gradient_norm=0.001, loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. Get number of samples and prepare data as numpy arrays\n",
    "    m = x.shape[0]\n",
    "    loss = []\n",
    "    x = np.array(x)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "    # 2. Augment features matrix with a column of ones for the bias (intercept)\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "\n",
    "    # 3. Initialize weights (theta) with zeros\n",
    "    w = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    thetas = [] # To store the history of weights for visualization/debugging\n",
    "\n",
    "    for i in range(itirations):\n",
    "        # 4. Hypothesis: Linear combination of features and weights (Predicted Y)\n",
    "        hx = x @ w\n",
    "        \n",
    "        # 5. Error: Difference between prediction and actual target\n",
    "        e = hx - y\n",
    "        \n",
    "        # 6. Cost Function: Mean Squared Error (MSE)\n",
    "        # Result is a 1x1 matrix (the \"inflated\" dimension you mentioned)\n",
    "        j = (e.T @ e) / (2 * m)\n",
    "        loss.append(j)\n",
    "        \n",
    "        # 7. Gradient: Compute the derivative of the cost function w.r.t weights\n",
    "        G = (x.T @ e) / m\n",
    "        \n",
    "        # Store current weights before update\n",
    "        thetas.append(w.copy())\n",
    "        \n",
    "        # 8. Convergence Check: Stop if the gradient magnitude is negligible\n",
    "        if np.linalg.norm(G) <= (gradient_norm):\n",
    "            print(f'Gradient Norm condition - Stopped at epoch {i}')\n",
    "            break\n",
    "            \n",
    "        # 9. Convergence Check: Stop if the improvement in loss is below threshold\n",
    "        if i > 2 and np.absolute(loss[i-1] - loss[i]) < loss_condition:\n",
    "            print(f'Loss condition - Stopped at epoch {i}')\n",
    "            break\n",
    "        \n",
    "        # 10. Parameter Update: Standard Gradient Descent rule\n",
    "        # Move weights in the opposite direction of the gradient\n",
    "        w = w - alpha * G\n",
    "\n",
    "    # 11. Post-training evaluation\n",
    "    hx_final = x @ w\n",
    "    print('Training Finished.')\n",
    "    print(f'Final R2 score = {r2_score(y, hx_final)}')\n",
    "    \n",
    "    return np.array(w), hx_final, np.array(loss), np.array(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e00bb2-874e-47a7-9642-d7b6559632ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss condition - Stopped at epoch 620\n",
      "Training Finished.\n",
      "Final R2 score = 0.9879310084690472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[1.62040000e-02],\n",
       "        [1.30758000e+00],\n",
       "        [1.31472000e+00],\n",
       "        [1.34670800e+00]],\n",
       "\n",
       "       [[6.48895936e-04],\n",
       "        [5.18495746e-02],\n",
       "        [5.20240698e-02],\n",
       "        [5.58824964e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[5.50917376e-03],\n",
       "        [4.96104648e-01],\n",
       "        [5.33387025e-01],\n",
       "        [9.87867423e-01]],\n",
       "\n",
       "       [[5.50290788e-03],\n",
       "        [4.95925224e-01],\n",
       "        [5.33310282e-01],\n",
       "        [9.88116866e-01]],\n",
       "\n",
       "       [[5.49663827e-03],\n",
       "        [4.95746035e-01],\n",
       "        [5.33233797e-01],\n",
       "        [9.88365831e-01]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,hx,loss,theta=BGD_LR(x,y,alpha=.0001)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca8dbefa-9d72-4e2f-9813-8887c2d3d4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  1.6204    ,   0.21837611,   0.21454457,   0.22443679],\n",
       "       [  3.224596  ,   0.43087585,   0.42331633,   0.44310425],\n",
       "       ...,\n",
       "       [161.98284409,   6.3959958 ,   6.78316925,  11.66856918],\n",
       "       [161.98341565,   6.39427156,   6.78244264,  11.67105836],\n",
       "       [161.98398149,   6.39254882,   6.78171769,  11.67354432]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "theta.shape=(-1,4)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a86933-ece7-46bf-b10a-cdd9ea20ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5668d7b-8ce9-436f-92bc-c110a1cb5648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1      2\n",
       "0  73.0  80.0   75.0\n",
       "1  93.0  88.0   93.0\n",
       "2  89.0  91.0   90.0\n",
       "3  96.0  98.0  100.0\n",
       "4  73.0  66.0   70.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8630d4a-7d68-48ea-9ef1-ec7643671a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  152.0\n",
       "1  185.0\n",
       "2  180.0\n",
       "3  196.0\n",
       "4  142.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8bc2645-9d0c-40db-b799-c24d5eb3d6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 73.,  66.,  70., 142.],\n",
       "       [ 96.,  98., 100., 196.],\n",
       "       [ 81.,  90.,  93., 183.],\n",
       "       [ 79.,  70.,  88., 164.],\n",
       "       [ 87.,  79.,  90., 175.],\n",
       "       [ 89.,  91.,  90., 180.],\n",
       "       [ 82.,  86.,  90., 177.],\n",
       "       [ 70.,  65.,  74., 141.],\n",
       "       [ 93.,  95.,  91., 184.],\n",
       "       [ 86.,  82.,  89., 175.],\n",
       "       [ 69.,  74.,  77., 149.],\n",
       "       [ 93.,  89.,  96., 192.],\n",
       "       [ 88.,  92.,  86., 177.],\n",
       "       [ 78.,  83.,  85., 175.],\n",
       "       [ 69.,  70.,  73., 141.],\n",
       "       [ 93.,  88.,  93., 185.],\n",
       "       [ 76.,  83.,  71., 149.],\n",
       "       [ 47.,  56.,  60., 115.],\n",
       "       [ 79.,  80.,  73., 152.],\n",
       "       [ 78.,  75.,  68., 147.],\n",
       "       [ 53.,  46.,  55., 101.],\n",
       "       [ 78.,  83.,  77., 159.],\n",
       "       [ 70.,  73.,  78., 148.],\n",
       "       [ 73.,  80.,  75., 152.],\n",
       "       [ 96.,  93.,  95., 192.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs=np.array(xs)\n",
    "y2=np.array(y2).reshape(-1,1)\n",
    "all_data=np.concatenate((x,y),axis=1)\n",
    "np.random.shuffle(all_data)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29fee3c5-05b1-4001-b1be-b8fcf748b31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 73.,  66.,  70.],\n",
       "        [ 96.,  98., 100.],\n",
       "        [ 81.,  90.,  93.],\n",
       "        [ 79.,  70.,  88.],\n",
       "        [ 87.,  79.,  90.],\n",
       "        [ 89.,  91.,  90.],\n",
       "        [ 82.,  86.,  90.],\n",
       "        [ 70.,  65.,  74.],\n",
       "        [ 93.,  95.,  91.],\n",
       "        [ 86.,  82.,  89.],\n",
       "        [ 69.,  74.,  77.],\n",
       "        [ 93.,  89.,  96.],\n",
       "        [ 88.,  92.,  86.],\n",
       "        [ 78.,  83.,  85.],\n",
       "        [ 69.,  70.,  73.],\n",
       "        [ 93.,  88.,  93.],\n",
       "        [ 76.,  83.,  71.],\n",
       "        [ 47.,  56.,  60.],\n",
       "        [ 79.,  80.,  73.],\n",
       "        [ 78.,  75.,  68.],\n",
       "        [ 53.,  46.,  55.],\n",
       "        [ 78.,  83.,  77.],\n",
       "        [ 70.,  73.,  78.],\n",
       "        [ 73.,  80.,  75.],\n",
       "        [ 96.,  93.,  95.]]),\n",
       " array([[142.],\n",
       "        [196.],\n",
       "        [183.],\n",
       "        [164.],\n",
       "        [175.],\n",
       "        [180.],\n",
       "        [177.],\n",
       "        [141.],\n",
       "        [184.],\n",
       "        [175.],\n",
       "        [149.],\n",
       "        [192.],\n",
       "        [177.],\n",
       "        [175.],\n",
       "        [141.],\n",
       "        [185.],\n",
       "        [149.],\n",
       "        [115.],\n",
       "        [152.],\n",
       "        [147.],\n",
       "        [101.],\n",
       "        [159.],\n",
       "        [148.],\n",
       "        [152.],\n",
       "        [192.]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs=all_data[:,:-1]\n",
    "y2=all_data[:,-1].reshape(-1,1)\n",
    "xs,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e106015-11a3-4e2c-b5a8-be7943b28d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBGD_LR(x,y,  alpha=0.05,batch_size=100 ,epochs=10 ,  gradient_norm=0.001  ,  loss_condition=0.001):\n",
    "    m=x.shape[0]\n",
    "    x=np.array(x)\n",
    "    y=np.array(y).reshape(-1,1)\n",
    "    all_data=np.concatenate((x,y),axis=1)\n",
    "    np.random.shuffle(all_data)\n",
    "    x=all_data[:,:-1]\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    y=all_data[:,-1].reshape(-1,1)\n",
    "    m=x.shape[0]\n",
    "    loss=[]\n",
    "    w=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    thetas=[] # return tensor \n",
    "    total_loss=[]\n",
    "    for i in range (epochs) :\n",
    "        start=0 \n",
    "        end= 0\n",
    "        total_epoch_loss=0\n",
    "        while(start<m):\n",
    "            end=start+batch_size if (start+batch_size)<m else m\n",
    "            x_temp=x[start:end,:]\n",
    "            y_temp=y[start:end,:]\n",
    "            m_temp=x_temp.shape[0]\n",
    "            hx=x_temp@w\n",
    "            e=hx-y_temp\n",
    "            j=e.T@e/(2*m_temp)\n",
    "            loss.append(j)\n",
    "            G=(x_temp.T@e)/m_temp\n",
    "            thetas.append(w.copy())\n",
    "            start=end\n",
    "            w=w-alpha*G\n",
    "            total_epoch_loss+=np.absolute(j)\n",
    "        total_loss.append(total_epoch_loss)\n",
    "        if np.linalg.norm(G)<=(gradient_norm):\n",
    "             print (f''' Gradient Norm condation\n",
    "             stop in epoch number {i} \n",
    "             gredient norm = {np.linalg.norm(G)}''')\n",
    "             break\n",
    "        if i>2 and np.absolute(total_loss[i-1]-total_loss[i])<loss_condition:\n",
    "            print (f'''loss condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={total_loss[i-1]-total_loss[i]}\n",
    "            ''')\n",
    "            break\n",
    "    y_mean = np.mean(y)\n",
    "    ss_res, ss_tot = 0, 0\n",
    "    for s in range(0, m, batch_size):\n",
    "        e = min(s + batch_size, m)\n",
    "        y_p = x[s:e, :] @ w\n",
    "        ss_res += np.sum((y[s:e] - y_p)**2)\n",
    "        ss_tot += np.sum((y[s:e] - y_mean)**2)\n",
    "    \n",
    "    print(f'Final R2 Score = {1 - (ss_res / ss_tot)}') \n",
    "    print('you finshed your itirations ')\n",
    "    \n",
    "    return np.array (w) ,np.array(loss),np.array(thetas)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf690a4b-2777-4186-8282-1478fb7e1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBGD_LR(x, y, alpha=0.05, batch_size=100, epochs=10, gradient_norm=0.001, loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. Basic preprocessing and data conversion\n",
    "    m = x.shape[0]\n",
    "    x = np.array(x)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "    # 2. Shuffle data at the start to ensure batches are representative and prevent bias\n",
    "    all_data = np.concatenate((x, y), axis=1)\n",
    "    np.random.shuffle(all_data)\n",
    "    x = all_data[:, :-1]\n",
    "    y = all_data[:, -1].reshape(-1, 1)\n",
    "\n",
    "    # 3. Add intercept (Bias) term\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    \n",
    "    # 4. Initialize parameters\n",
    "    w = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    loss = []         # Stores loss for every single batch update\n",
    "    thetas = []       # Stores weight history\n",
    "    total_loss = []   # Stores the accumulated loss per epoch\n",
    "\n",
    "    for i in range(epochs):\n",
    "        start = 0\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        # 5. Mini-batch loop: Iterate over the dataset in small chunks\n",
    "        while start < m:\n",
    "            # Determine the end index of the current batch\n",
    "            end = start + batch_size if (start + batch_size) < m else m\n",
    "            x_temp = x[start:end, :]\n",
    "            y_temp = y[start:end, :]\n",
    "            m_temp = x_temp.shape[0]\n",
    "\n",
    "            # 6. Forward pass on the mini-batch\n",
    "            hx = x_temp @ w\n",
    "            e = hx - y_temp\n",
    "            \n",
    "            # 7. Compute Batch Loss (Cost)\n",
    "            j = (e.T @ e) / (2 * m_temp)\n",
    "            loss.append(j)\n",
    "            \n",
    "            # 8. Compute Gradient based ONLY on the current mini-batch\n",
    "            G = (x_temp.T @ e) / m_temp\n",
    "            thetas.append(w.copy())\n",
    "            \n",
    "            # 9. Update Weights (Standard GD rule applied per batch)\n",
    "            w = w - alpha * G\n",
    "            \n",
    "            # Move to the next batch\n",
    "            start = end\n",
    "            \n",
    "            # Accumulate loss to monitor epoch-level performance\n",
    "            total_epoch_loss += np.absolute(j)\n",
    "            \n",
    "        total_loss.append(total_epoch_loss)\n",
    "\n",
    "        # 10. Convergence Check: Gradient Norm (from the last batch of the epoch)\n",
    "        if np.linalg.norm(G) <= gradient_norm:\n",
    "            print(f'Gradient Norm Condition - Stopped at epoch {i}')\n",
    "            break\n",
    "            \n",
    "        # 11. Convergence Check: Stability of total epoch loss\n",
    "        if i > 2 and np.absolute(total_loss[i-1] - total_loss[i]) < loss_condition:\n",
    "            print(f'Loss Condition - Stopped at epoch {i}')\n",
    "            break\n",
    "\n",
    "    # 12. Evaluation: Manual R2 Score calculation across the entire dataset\n",
    "    y_mean = np.mean(y)\n",
    "    ss_res, ss_tot = 0, 0\n",
    "    for s in range(0, m, batch_size):\n",
    "        e = min(s + batch_size, m)\n",
    "        y_p = x[s:e, :] @ w\n",
    "        ss_res += np.sum((y[s:e] - y_p)**2)\n",
    "        ss_tot += np.sum((y[s:e] - y_mean)**2)\n",
    "    \n",
    "    print(f'Final R2 Score = {1 - (ss_res / ss_tot)}') \n",
    "    print('Training Finished.')\n",
    "    \n",
    "    return np.array(w), np.array(loss), np.array(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f422acb-a774-4d44-a3f1-b6e44054754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final R2 Score = 0.9795736653431664\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "w,loss,theta=MBGD_LR(x,y,alpha=.00001,batch_size=10,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1a62ff1d-ed19-448c-a439-0740c67991ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.array(loss).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a9550ec-08b3-4505-8e4d-906daa983f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24eac244470>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGdCAYAAAAbudkLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4xklEQVR4nO3df3hU5Z3//9fMJBlCmpwS0mSY8qN0l6VokNJoQ9CutGCKa0j9+N2lNjalV/lCXQQ2K1Rlu22x328JxRbabVZF26vYLTW9rq/GddVG0q1ic/HTaFYB8cdK+WVCUJMJgZCEmfv7B8yBIQiJzDknJs/Hdc11yTnvObnPzcR5cZ/73MdnjDECAAAYhPxeNwAAAMApBB0AADBoEXQAAMCgRdABAACDFkEHAAAMWgQdAAAwaBF0AADAoEXQAQAAg1aK1w3wUiwW0zvvvKPMzEz5fD6vmwMAAPrAGKNjx44pHA7L77/4mM2QDjrvvPOOxowZ43UzAADAh3Dw4EGNHj36ojVDOuhkZmZKOt1RWVlZHrcGAAD0RXt7u8aMGWN/j1/MkA468ctVWVlZBB0AAD5i+jLtpN+TkV944QXNmTNH4XBYPp9PTzzxxAfWfvvb35bP59PPfvazhO1dXV1asmSJcnJylJGRodLSUh06dCihprW1VeXl5bIsS5Zlqby8XG1tbQk1Bw4c0Jw5c5SRkaGcnBwtXbpU3d3d/T0lAAAwSPU76Bw/flxTpkxRVVXVReueeOIJbd++XeFwuNe+iooK1dTUqLq6WvX19ero6FBJSYmi0ahdU1ZWpsbGRtXW1qq2tlaNjY0qLy+390ejUd100006fvy46uvrVV1drccee0zLli3r7ykBAIDBylwGSaampqbX9kOHDplPfvKTZteuXWbcuHFm3bp19r62tjaTmppqqqur7W2HDx82fr/f1NbWGmOM2bNnj5Fktm3bZtds3brVSDJ79+41xhjzzDPPGL/fbw4fPmzXPProoyYYDJpIJNKn9kciESOpz/UAAMB7/fn+Tvo6OrFYTOXl5frOd76jK6+8stf+hoYG9fT0qLi42N4WDoeVn5+vLVu2SJK2bt0qy7JUWFho10ybNk2WZSXU5OfnJ4wYffnLX1ZXV5caGhou2Lauri61t7cnvAAAwOCV9KDz4x//WCkpKVq6dOkF9zc3NystLU0jRoxI2J6Xl6fm5ma7Jjc3t9d7c3NzE2ry8vIS9o8YMUJpaWl2zfkqKyvtOT+WZXFrOQAAg1xSg05DQ4N+/vOfa8OGDf1egM8Yk/CeC73/w9Sca8WKFYpEIvbr4MGD/WojAAD4aElq0Pnzn/+slpYWjR07VikpKUpJSdH+/fu1bNkyfepTn5IkhUIhdXd3q7W1NeG9LS0t9ghNKBTSkSNHeh3/6NGjCTXnj9y0traqp6en10hPXDAYtG8l55ZyAAAGv6QGnfLycr3yyitqbGy0X+FwWN/5znf07LPPSpIKCgqUmpqquro6+31NTU3atWuXpk+fLkkqKipSJBLRjh077Jrt27crEokk1OzatUtNTU12zaZNmxQMBlVQUJDM0wIAAB9R/V4wsKOjQ2+99Zb953379qmxsVHZ2dkaO3asRo4cmVCfmpqqUCikiRMnSpIsy9L8+fO1bNkyjRw5UtnZ2Vq+fLkmT56sWbNmSZImTZqk2bNna8GCBVq/fr0kaeHChSopKbGPU1xcrCuuuELl5eW677779P7772v58uVasGABIzUAAEDShxjRefHFFzV16lRNnTpVknTnnXdq6tSp+v73v9/nY6xbt04333yz5s6dq2uvvVbDhw/Xf/3XfykQCNg1Gzdu1OTJk1VcXKzi4mJdddVV+o//+A97fyAQ0NNPP61hw4bp2muv1dy5c3XzzTfrJz/5SX9PCQAADFI+Y4zxuhFeaW9vl2VZikQijAIBAPAR0Z/v76TfXg4AADBQDOmHejrlxb+8r6deadJnQpm69fNjvW4OAABDFiM6Dnj9yDFt2PIX/ffeFq+bAgDAkEbQcYD/zIKFQ3j6EwAAAwJBxwGBM0EnGiPoAADgJYKOA/z+00GHnAMAgLcIOg44k3MU49IVAACeIug4IGCP6BB0AADwEkHHAT7m6AAAMCAQdBwQn4xMzgEAwFsEHQfYc3RIOgAAeIqg4wA/c3QAABgQCDoOiC8YGCXnAADgKYKOAwJnepVLVwAAeIug4wC/j0tXAAAMBAQdB/i5vRwAgAGBoOOA+IKBDOgAAOAtgo4DzgzoKErSAQDAUwQdBwSYowMAwIBA0HGAvY4Oc3QAAPAUQccBfh4BAQDAgEDQcUD8ERDcdQUAgLcIOg4I8AgIAAAGBIKOA1gwEACAgYGg44CzCwZ63BAAAIY4go4Dzi4YyIgOAABeIug4wM+CgQAADAgEHQewjg4AAAMDQccBrKMDAMDAQNBxAI+AAABgYCDoOMB/pldZMBAAAG8RdBzAOjoAAAwMBB0HnF0Z2eOGAAAwxBF0HODjWVcAAAwIBB0HxCcjSywaCACAlwg6DvCfE3QY1QEAwDsEHQfEFwyUmKcDAICXCDoOOCfncOcVAAAe6nfQeeGFFzRnzhyFw2H5fD498cQT9r6enh7dfffdmjx5sjIyMhQOh/WNb3xD77zzTsIxurq6tGTJEuXk5CgjI0OlpaU6dOhQQk1ra6vKy8tlWZYsy1J5ebna2toSag4cOKA5c+YoIyNDOTk5Wrp0qbq7u/t7SkkXSBjRIegAAOCVfged48ePa8qUKaqqquq178SJE3rppZf0ve99Ty+99JIef/xxvfHGGyotLU2oq6ioUE1Njaqrq1VfX6+Ojg6VlJQoGo3aNWVlZWpsbFRtba1qa2vV2Nio8vJye380GtVNN92k48ePq76+XtXV1Xrssce0bNmy/p5S0jFHBwCAAcJcBkmmpqbmojU7duwwksz+/fuNMca0tbWZ1NRUU11dbdccPnzY+P1+U1tba4wxZs+ePUaS2bZtm12zdetWI8ns3bvXGGPMM888Y/x+vzl8+LBd8+ijj5pgMGgikUif2h+JRIykPtf3VVdP1Iy7+ykz7u6nTNvx7qQeGwCAoa4/39+Oz9GJRCLy+Xz6+Mc/LklqaGhQT0+PiouL7ZpwOKz8/Hxt2bJFkrR161ZZlqXCwkK7Ztq0abIsK6EmPz9f4XDYrvnyl7+srq4uNTQ0XLAtXV1dam9vT3g5gUtXAAAMDI4GnZMnT+qee+5RWVmZsrKyJEnNzc1KS0vTiBEjEmrz8vLU3Nxs1+Tm5vY6Xm5ubkJNXl5ewv4RI0YoLS3NrjlfZWWlPefHsiyNGTPmss/xQs6djBwl6AAA4BnHgk5PT49uvfVWxWIx3X///ZesN8bId87clnP/+3JqzrVixQpFIhH7dfDgwb6cSr/5fD57dWRGdAAA8I4jQaenp0dz587Vvn37VFdXZ4/mSFIoFFJ3d7daW1sT3tPS0mKP0IRCIR05cqTXcY8ePZpQc/7ITWtrq3p6enqN9MQFg0FlZWUlvJxiP9gz5tiPAAAAl5D0oBMPOW+++ab++Mc/auTIkQn7CwoKlJqaqrq6OntbU1OTdu3apenTp0uSioqKFIlEtGPHDrtm+/btikQiCTW7du1SU1OTXbNp0yYFg0EVFBQk+7T6LcATzAEA8FxKf9/Q0dGht956y/7zvn371NjYqOzsbIXDYf393/+9XnrpJT311FOKRqP2qEt2drbS0tJkWZbmz5+vZcuWaeTIkcrOztby5cs1efJkzZo1S5I0adIkzZ49WwsWLND69eslSQsXLlRJSYkmTpwoSSouLtYVV1yh8vJy3XfffXr//fe1fPlyLViwwNGRmr7iwZ4AAAwA/b2l67nnnjOSer3mzZtn9u3bd8F9ksxzzz1nH6Ozs9MsXrzYZGdnm/T0dFNSUmIOHDiQ8HPee+89c9ttt5nMzEyTmZlpbrvtNtPa2ppQs3//fnPTTTeZ9PR0k52dbRYvXmxOnjzZ53Nx6vZyY4yZ9L0/mHF3P2X2v3s86ccGAGAo68/3t8+YoXttpb29XZZlKRKJJH0UaPIPntWxrlN6bvkMjc/JSOqxAQAYyvrz/c2zrhzCXVcAAHiPoOOQ+KKBMeboAADgGYKOQ+K3l7NgIAAA3iHoOMTvZx0dAAC8RtBxiJ85OgAAeI6g4xAWDAQAwHsEHYfEn7fFgoEAAHiHoOMQ+64rcg4AAJ4h6DjkbNAh6QAA4BWCjkPsBQMZ0gEAwDMEHYcEWEcHAADPEXQcEl8wkHV0AADwDkHHIX7m6AAA4DmCjkPiCwZy6QoAAO8QdBwSv+vKEHQAAPAMQcchZxcM9LghAAAMYQQdhwR41hUAAJ4j6DjEXjCQdXQAAPAMQcchPh+PgAAAwGsEHYewYCAAAN4j6DjEf6ZnuXQFAIB3CDoOsVdGZkQHAADPEHQc4rdvLyfoAADgFYKOQ84uGOhxQwAAGMIIOg7hERAAAHiPoOMQ5ugAAOA9go5DWDAQAADvEXQc4mfBQAAAPEfQcYjfz11XAAB4jaDjED8P9QQAwHMEHYcEmIwMAIDnCDoO8dkLBnrcEAAAhjCCjkMC8WddMaIDAIBnCDoOse+6YjIyAACeIeg4JH7XFTkHAADvEHQcEp+MzCMgAADwDkHHIfHbyw1BBwAAzxB0HMKCgQAAeI+g4xAeAQEAgPf6HXReeOEFzZkzR+FwWD6fT0888UTCfmOMVq5cqXA4rPT0dM2YMUO7d+9OqOnq6tKSJUuUk5OjjIwMlZaW6tChQwk1ra2tKi8vl2VZsixL5eXlamtrS6g5cOCA5syZo4yMDOXk5Gjp0qXq7u7u7yk5wn6oJ5euAADwTL+DzvHjxzVlyhRVVVVdcP+aNWu0du1aVVVVaefOnQqFQrrhhht07Ngxu6aiokI1NTWqrq5WfX29Ojo6VFJSomg0ateUlZWpsbFRtbW1qq2tVWNjo8rLy+390WhUN910k44fP676+npVV1frscce07Jly/p7So44M6DDpSsAALxkLoMkU1NTY/85FouZUChkVq9ebW87efKksSzLPPjgg8YYY9ra2kxqaqqprq62aw4fPmz8fr+pra01xhizZ88eI8ls27bNrtm6dauRZPbu3WuMMeaZZ54xfr/fHD582K559NFHTTAYNJFIpE/tj0QiRlKf6/vjx394zYy7+ymz8sldST82AABDWX++v5M6R2ffvn1qbm5WcXGxvS0YDOr666/Xli1bJEkNDQ3q6elJqAmHw8rPz7drtm7dKsuyVFhYaNdMmzZNlmUl1OTn5yscDts1X/7yl9XV1aWGhoYLtq+rq0vt7e0JL6ewYCAAAN5LatBpbm6WJOXl5SVsz8vLs/c1NzcrLS1NI0aMuGhNbm5ur+Pn5uYm1Jz/c0aMGKG0tDS75nyVlZX2nB/LsjRmzJgPcZZ9w4KBAAB4z5G7ruIPtIwzxvTadr7zay5U/2FqzrVixQpFIhH7dfDgwYu26XKwYCAAAN5LatAJhUKS1GtEpaWlxR59CYVC6u7uVmtr60Vrjhw50uv4R48eTag5/+e0traqp6en10hPXDAYVFZWVsLLKSwYCACA95IadMaPH69QKKS6ujp7W3d3tzZv3qzp06dLkgoKCpSamppQ09TUpF27dtk1RUVFikQi2rFjh12zfft2RSKRhJpdu3apqanJrtm0aZOCwaAKCgqSeVofCgsGAgDgvZT+vqGjo0NvvfWW/ed9+/apsbFR2dnZGjt2rCoqKrRq1SpNmDBBEyZM0KpVqzR8+HCVlZVJkizL0vz587Vs2TKNHDlS2dnZWr58uSZPnqxZs2ZJkiZNmqTZs2drwYIFWr9+vSRp4cKFKikp0cSJEyVJxcXFuuKKK1ReXq777rtP77//vpYvX64FCxY4OlLTVywYCACA9/oddF588UV98YtftP985513SpLmzZunDRs26K677lJnZ6cWLVqk1tZWFRYWatOmTcrMzLTfs27dOqWkpGju3Lnq7OzUzJkztWHDBgUCAbtm48aNWrp0qX13VmlpacLaPYFAQE8//bQWLVqka6+9Vunp6SorK9NPfvKT/veCAwJnxsq46woAAO/4zBCeRNLe3i7LshSJRJI+CvTLP7+t//fp1/SVz4b181unJvXYAAAMZf35/uZZVw7h0hUAAN4j6DgkftcVl64AAPAOQcchPNQTAADvEXQcwu3lAAB4j6DjEOboAADgPYKOQwI+Ll0BAOA1go5D4o/bIugAAOAdgo5DAszRAQDAcwQdh/i5dAUAgOcIOg6J33UVi3ncEAAAhjCCjkPiCwZGGdEBAMAzBB2HxO+6GsKPEgMAwHMEHYewYCAAAN4j6DiEBQMBAPAeQcchgTM9y11XAAB4h6DjEB+3lwMA4DmCjkPik5Gj3F4OAIBnCDoO8XPXFQAAniPoOMR/pme56woAAO8QdBwSH9FhwUAAALxD0HFI/KGe5BwAALxD0HGIPaLDpSsAADxD0HFI/FlX3F4OAIB3CDoOCdhPLyfoAADgFYKOQ3gEBAAA3iPoOIS7rgAA8B5BxyHxdXRYMBAAAO8QdBwS4K4rAAA8R9BxiN9P0AEAwGsEHYecfdaVxw0BAGAII+g4JMBkZAAAPEfQcYiPBQMBAPAcQcchZxcM9LghAAAMYQQdh5xdMJARHQAAvELQcUh8HR3m6AAA4B2CjkPOveuKRQMBAPAGQcch8buuJJ53BQCAVwg6DokvGCixaCAAAF5JetA5deqU/vVf/1Xjx49Xenq6Pv3pT+uHP/yhYufcfmSM0cqVKxUOh5Wenq4ZM2Zo9+7dCcfp6urSkiVLlJOTo4yMDJWWlurQoUMJNa2trSovL5dlWbIsS+Xl5Wpra0v2KX0o5+QcJiQDAOCRpAedH//4x3rwwQdVVVWl1157TWvWrNF9992nX/ziF3bNmjVrtHbtWlVVVWnnzp0KhUK64YYbdOzYMbumoqJCNTU1qq6uVn19vTo6OlRSUqJoNGrXlJWVqbGxUbW1taqtrVVjY6PKy8uTfUofSsB/7qUrgg4AAJ4wSXbTTTeZb33rWwnbbrnlFvP1r3/dGGNMLBYzoVDIrF692t5/8uRJY1mWefDBB40xxrS1tZnU1FRTXV1t1xw+fNj4/X5TW1trjDFmz549RpLZtm2bXbN161Yjyezdu7dPbY1EIkaSiUQiH+5kL6Kz+5QZd/dTZtzdT5ljJ3uSfnwAAIaq/nx/J31E57rrrtN///d/64033pAk/c///I/q6+v1d3/3d5Kkffv2qbm5WcXFxfZ7gsGgrr/+em3ZskWS1NDQoJ6enoSacDis/Px8u2br1q2yLEuFhYV2zbRp02RZll1zvq6uLrW3tye8nOL3MUcHAACvpST7gHfffbcikYg+85nPKBAIKBqN6kc/+pG+9rWvSZKam5slSXl5eQnvy8vL0/79++2atLQ0jRgxoldN/P3Nzc3Kzc3t9fNzc3PtmvNVVlbq3nvvvbwT7KNz5+gYLl0BAOCJpI/o/P73v9dvf/tb/e53v9NLL72kRx55RD/5yU/0yCOPJNT5zhnxkE6HgfO3ne/8mgvVX+w4K1asUCQSsV8HDx7s62n1W4C7rgAA8FzSR3S+853v6J577tGtt94qSZo8ebL279+vyspKzZs3T6FQSNLpEZlRo0bZ72tpabFHeUKhkLq7u9Xa2powqtPS0qLp06fbNUeOHOn1848ePdprtCguGAwqGAwm50Qvwcc6OgAAeC7pIzonTpyQ35942EAgYN9ePn78eIVCIdXV1dn7u7u7tXnzZjvEFBQUKDU1NaGmqalJu3btsmuKiooUiUS0Y8cOu2b79u2KRCJ2jdfsB3ty6QoAAE8kfURnzpw5+tGPfqSxY8fqyiuv1Msvv6y1a9fqW9/6lqTTIx0VFRVatWqVJkyYoAkTJmjVqlUaPny4ysrKJEmWZWn+/PlatmyZRo4cqezsbC1fvlyTJ0/WrFmzJEmTJk3S7NmztWDBAq1fv16StHDhQpWUlGjixInJPq0PJeDzKSpD0AEAwCNJDzq/+MUv9L3vfU+LFi1SS0uLwuGwvv3tb+v73/++XXPXXXeps7NTixYtUmtrqwoLC7Vp0yZlZmbaNevWrVNKSormzp2rzs5OzZw5Uxs2bFAgELBrNm7cqKVLl9p3Z5WWlqqqqirZp/Shxa9eMUcHAABv+MwQviWovb1dlmUpEokoKysr6ce/4vu1OtEd1Qvf+aLGjhye9OMDADAU9ef7m2ddOSi+lg6XrgAA8AZBx0HxO8yjBB0AADxB0HFQ/AnmQ/jqIAAAniLoOChw5tJVNHaJQgAA4AiCjoN8zNEBAMBTBB0HpQVOB50T3ac8bgkAAEMTQcdBk0advuXtpf1t3jYEAIAhiqDjoGmfHilJ2vb2ex63BACAoYmg46DCT2dLknb85X1WRwYAwAMEHQddMSpLHwum6NjJU3qtqd3r5gAAMOQQdByUEvDrmk+NkMTlKwAAvEDQcdjnx5+ep/PygTZvGwIAwBBE0HHYJzKDkqTj3GIOAIDrCDoOS/HHV0dmMjIAAG4j6DgscCbonIoSdAAAcBtBx2GpAUZ0AADwCkHHYQH/6S4+FePJngAAuI2g47D4HJ1TjOgAAOA6go7DmKMDAIB3CDoO464rAAC8Q9BxmD2iwxwdAABcR9BxWAp3XQEA4BmCjsPO3nVF0AEAwG0EHYcxRwcAAO8QdBwWv3TFiA4AAO4j6DiMER0AALxD0HFYfI5OT5S7rgAAcBtBx2GM6AAA4B2CjsMCPAICAADPEHQcxogOAADeIeg4LHBO0DGGsAMAgJsIOg5L8Z/tYkZ1AABwF0HHYYEz6+hIzNMBAMBtBB2HxefoSIzoAADgNoKOw84NOozoAADgLoKOwwLnBh0WDQQAwFUEHYf5fL6EO68AAIB7CDouYNFAAAC84UjQOXz4sL7+9a9r5MiRGj58uD772c+qoaHB3m+M0cqVKxUOh5Wenq4ZM2Zo9+7dCcfo6urSkiVLlJOTo4yMDJWWlurQoUMJNa2trSovL5dlWbIsS+Xl5Wpra3PilC4LiwYCAOCNpAed1tZWXXvttUpNTdUf/vAH7dmzRz/96U/18Y9/3K5Zs2aN1q5dq6qqKu3cuVOhUEg33HCDjh07ZtdUVFSopqZG1dXVqq+vV0dHh0pKShSNRu2asrIyNTY2qra2VrW1tWpsbFR5eXmyT+myMaIDAIBHTJLdfffd5rrrrvvA/bFYzIRCIbN69Wp728mTJ41lWebBBx80xhjT1tZmUlNTTXV1tV1z+PBh4/f7TW1trTHGmD179hhJZtu2bXbN1q1bjSSzd+/ePrU1EokYSSYSifTrHPvrs/c+a8bd/ZR580i7oz8HAIChoD/f30kf0XnyySd19dVX6x/+4R+Um5urqVOn6uGHH7b379u3T83NzSouLra3BYNBXX/99dqyZYskqaGhQT09PQk14XBY+fn5ds3WrVtlWZYKCwvtmmnTpsmyLLtmoAicWR2ZER0AANyV9KDz9ttv64EHHtCECRP07LPP6vbbb9fSpUv1m9/8RpLU3NwsScrLy0t4X15enr2vublZaWlpGjFixEVrcnNze/383Nxcu+Z8XV1dam9vT3i5IT5H51SUoAMAgJtSkn3AWCymq6++WqtWrZIkTZ06Vbt379YDDzygb3zjG3adz+dLeJ8xpte2851fc6H6ix2nsrJS9957b5/PJVlSAkxGBgDAC0kf0Rk1apSuuOKKhG2TJk3SgQMHJEmhUEiSeo26tLS02KM8oVBI3d3dam1tvWjNkSNHev38o0eP9hotiluxYoUikYj9Onjw4Ic4w/6zR3RiLBgIAICbkh50rr32Wr3++usJ29544w2NGzdOkjR+/HiFQiHV1dXZ+7u7u7V582ZNnz5dklRQUKDU1NSEmqamJu3atcuuKSoqUiQS0Y4dO+ya7du3KxKJ2DXnCwaDysrKSni5IcClKwAAPJH0S1f//M//rOnTp2vVqlWaO3euduzYoYceekgPPfSQpNOXmyoqKrRq1SpNmDBBEyZM0KpVqzR8+HCVlZVJkizL0vz587Vs2TKNHDlS2dnZWr58uSZPnqxZs2ZJOj1KNHv2bC1YsEDr16+XJC1cuFAlJSWaOHFisk/rsqScmYzMpSsAANyV9KBzzTXXqKamRitWrNAPf/hDjR8/Xj/72c9022232TV33XWXOjs7tWjRIrW2tqqwsFCbNm1SZmamXbNu3TqlpKRo7ty56uzs1MyZM7VhwwYFAgG7ZuPGjVq6dKl9d1ZpaamqqqqSfUqXjXV0AADwhs8YM2S/fdvb22VZliKRiKOXsUqr6vXKoYh+/c1r9MXP9L5TDAAA9F1/vr951pULGNEBAMAbBB0XnH3WFXddAQDgJoKOCxjRAQDAGwQdF3DXFQAA3iDouCC+MnIP6+gAAOAqgo4LmKMDAIA3CDouYI4OAADeIOi4gDk6AAB4g6DjAp51BQCANwg6Ljg7R4egAwCAmwg6LmCODgAA3iDouCB+ezl3XQEA4C6CjgviIzqsowMAgLsIOi7grisAALxB0HFBCnN0AADwBEHHBQHm6AAA4AmCjgsY0QEAwBsEHRcEmKMDAIAnCDouYEQHAABvEHRcEL+9PMrt5QAAuIqg4wJGdAAA8AZBxwVnHwHBXVcAALiJoOOC1MDpbmZEBwAAdxF0XMAcHQAAvEHQcQFzdAAA8AZBxwX2iA5zdAAAcBVBxwUpAUZ0AADwAkHHBayMDACANwg6LmCODgAA3iDouODsHB2CDgAAbiLouMAe0YkyGRkAADcRdFwQ4NIVAACeIOi4IL4yMpeuAABwF0HHBYzoAADgDYKOC1KYjAwAgCcIOi7g6eUAAHiDoOOClPiCgTzUEwAAVxF0XMAcHQAAvEHQcUH8WVfM0QEAwF2OB53Kykr5fD5VVFTY24wxWrlypcLhsNLT0zVjxgzt3r074X1dXV1asmSJcnJylJGRodLSUh06dCihprW1VeXl5bIsS5Zlqby8XG1tbU6fUr/FR3R6WDAQAABXORp0du7cqYceekhXXXVVwvY1a9Zo7dq1qqqq0s6dOxUKhXTDDTfo2LFjdk1FRYVqampUXV2t+vp6dXR0qKSkRNFo1K4pKytTY2OjamtrVVtbq8bGRpWXlzt5Sh8Kd10BAOANx4JOR0eHbrvtNj388MMaMWKEvd0Yo5/97Gf67ne/q1tuuUX5+fl65JFHdOLECf3ud7+TJEUiEf3qV7/ST3/6U82aNUtTp07Vb3/7W7366qv64x//KEl67bXXVFtbq1/+8pcqKipSUVGRHn74YT311FN6/fXXnTqtDyXlzIKBzNEBAMBdjgWdO+64QzfddJNmzZqVsH3fvn1qbm5WcXGxvS0YDOr666/Xli1bJEkNDQ3q6elJqAmHw8rPz7drtm7dKsuyVFhYaNdMmzZNlmXZNefr6upSe3t7wssNjOgAAOCNFCcOWl1drZdeekk7d+7sta+5uVmSlJeXl7A9Ly9P+/fvt2vS0tISRoLiNfH3Nzc3Kzc3t9fxc3Nz7ZrzVVZW6t577+3/CV2mc++6MsbI5/O53gYAAIaipI/oHDx4UP/0T/+k3/72txo2bNgH1p3/Zd+XAHB+zYXqL3acFStWKBKJ2K+DBw9e9OclS3xER5IY1AEAwD1JDzoNDQ1qaWlRQUGBUlJSlJKSos2bN+vf/u3flJKSYo/knD/q0tLSYu8LhULq7u5Wa2vrRWuOHDnS6+cfPXq012hRXDAYVFZWVsLLDYFzgg6rIwMA4J6kB52ZM2fq1VdfVWNjo/26+uqrddttt6mxsVGf/vSnFQqFVFdXZ7+nu7tbmzdv1vTp0yVJBQUFSk1NTahpamrSrl277JqioiJFIhHt2LHDrtm+fbsikYhdM1DEV0aWmKcDAICbkj5HJzMzU/n5+QnbMjIyNHLkSHt7RUWFVq1apQkTJmjChAlatWqVhg8frrKyMkmSZVmaP3++li1bppEjRyo7O1vLly/X5MmT7cnNkyZN0uzZs7VgwQKtX79ekrRw4UKVlJRo4sSJyT6ty5I4okPQAQDALY5MRr6Uu+66S52dnVq0aJFaW1tVWFioTZs2KTMz065Zt26dUlJSNHfuXHV2dmrmzJnasGGDAoGAXbNx40YtXbrUvjurtLRUVVVVrp/PpZw7R+cUz7sCAMA1PmPMkP3mbW9vl2VZikQijs/XGb/iaRkj7fjuTOVmfvAkbQAAcHH9+f7mWVcuYS0dAADcR9BxSXxCMpeuAABwD0HHJYzoAADgPoKOSwKBs6sjAwAAdxB0XMKIDgAA7iPouOTs865YGRkAALcQdFwSn4zcw2RkAABcQ9BxybDU0119sifqcUsAABg6CDouSU87vaJzJ0EHAADXEHRcMjz19NM2OrsJOgAAuIWg45Jh8REdgg4AAK4h6Lgk/cwcHS5dAQDgHoKOS4ancekKAAC3EXRcMiyVycgAALiNoOOSdIIOAACuI+i4ZDiTkQEAcB1BxyXpBB0AAFxH0HEJc3QAAHAfQccl8UtXJxjRAQDANQQdl8QnI/OsKwAA3EPQcQmXrgAAcB9BxyVcugIAwH0EHZfE77ri0hUAAO4h6LgkPkfnRPcpj1sCAMDQQdBxCevoAADgPoKOS87edRXzuCUAAAwdBB2XxINOdzSmU1HCDgAAbiDouCR+6UriFnMAANxC0HFJMMUvn+/0fxN0AABwB0HHJT6fz758xYRkAADcQdBxUXzRQEZ0AABwB0HHRcMY0QEAwFUEHRdx6QoAAHcRdFzEpSsAANxF0HERTzAHAMBdBB0XpfMEcwAAXEXQcdFwnmAOAICrCDouGpbKiA4AAG5KetCprKzUNddco8zMTOXm5urmm2/W66+/nlBjjNHKlSsVDoeVnp6uGTNmaPfu3Qk1XV1dWrJkiXJycpSRkaHS0lIdOnQooaa1tVXl5eWyLEuWZam8vFxtbW3JPqWk4a4rAADclfSgs3nzZt1xxx3atm2b6urqdOrUKRUXF+v48eN2zZo1a7R27VpVVVVp586dCoVCuuGGG3Ts2DG7pqKiQjU1NaqurlZ9fb06OjpUUlKiaPRsSCgrK1NjY6Nqa2tVW1urxsZGlZeXJ/uUkoZLVwAAuMw4rKWlxUgymzdvNsYYE4vFTCgUMqtXr7ZrTp48aSzLMg8++KAxxpi2tjaTmppqqqur7ZrDhw8bv99vamtrjTHG7Nmzx0gy27Zts2u2bt1qJJm9e/f2qW2RSMRIMpFI5LLPsy9++uxeM+7up8y/1rzqys8DAGAw6s/3t+NzdCKRiCQpOztbkrRv3z41NzeruLjYrgkGg7r++uu1ZcsWSVJDQ4N6enoSasLhsPLz8+2arVu3yrIsFRYW2jXTpk2TZVl2zfm6urrU3t6e8HLTMNbRAQDAVY4GHWOM7rzzTl133XXKz8+XJDU3N0uS8vLyEmrz8vLsfc3NzUpLS9OIESMuWpObm9vrZ+bm5to156usrLTn81iWpTFjxlzeCfbTcNbRAQDAVY4GncWLF+uVV17Ro48+2mufz+dL+LMxpte2851fc6H6ix1nxYoVikQi9uvgwYN9OY2kia+jc/D9E4rFjKs/GwCAocixoLNkyRI9+eSTeu655zR69Gh7eygUkqReoy4tLS32KE8oFFJ3d7daW1svWnPkyJFeP/fo0aO9RovigsGgsrKyEl5uKhw/UsEUv145FNHaujdc/dkAAAxFSQ86xhgtXrxYjz/+uP70pz9p/PjxCfvHjx+vUCikuro6e1t3d7c2b96s6dOnS5IKCgqUmpqaUNPU1KRdu3bZNUVFRYpEItqxY4dds337dkUiEbtmoPlUToZW/1+TJUlVz72lv7x7/BLvAAAAlyMl2Qe844479Lvf/U7/+Z//qczMTHvkxrIspaeny+fzqaKiQqtWrdKECRM0YcIErVq1SsOHD1dZWZldO3/+fC1btkwjR45Udna2li9frsmTJ2vWrFmSpEmTJmn27NlasGCB1q9fL0lauHChSkpKNHHixGSfVtL8n6mj9eDzb+v1I8d04P0T+lROhtdNAgBg0Ep60HnggQckSTNmzEjY/utf/1rf/OY3JUl33XWXOjs7tWjRIrW2tqqwsFCbNm1SZmamXb9u3TqlpKRo7ty56uzs1MyZM7VhwwYFAgG7ZuPGjVq6dKl9d1ZpaamqqqqSfUpJZw1PlSQdO3nK45YAADC4+YwxQ3ZWbHt7uyzLUiQScXW+zv/9yIv642tHVHnLZH3t82Nd+7kAAAwG/fn+5llXHshKPz2Q1t7Z43FLAAAY3Ag6HsgadvrSVftJgg4AAE4i6Hgga1h8RIc5OgAAOImg44Gs9PhkZEZ0AABwEkHHA2cvXTGiAwCAkwg6HmAyMgAA7iDoeCCTycgAALiCoOOB+KUrFgwEAMBZBB0PcOkKAAB3EHQ8EB/ROd4d1alozOPWAAAweBF0PJA57Owjxrh8BQCAcwg6HkgJ+DU87fTDSZmQDACAcwg6HmFCMgAAziPoeIQJyQAAOI+g4xEe7AkAgPMIOh6JP++KB3sCAOAcgo5H4ndeMaIDAIBzCDoe4cGeAAA4j6DjESYjAwDgPIKOR5iMDACA8wg6HmEyMgAAziPoeGRkRpokqbm90+OWAAAweBF0PDIhL1OS9FZLh2Ix43FrAAAYnAg6HhmbPVzBFL9O9sR0sPWE180BAGBQIuh4JOD36a8+8TFJ0uvNxzxuDQAAgxNBx0MTQ6cvX73Z0uFxSwAAGJwIOh6akMeIDgAATiLoeOhvck+P6LxxhKADAIATCDoeil+6evvocZ2KxjxuDQAAgw9Bx0Of/Hi60lMD6o7GtLbuDUVOsEoyAADJRNDxkN/v03UTciRJ9z//v1r+//2Pxy0CAGBwIeh47N/LPqf/5+Z8SdLW/31PURYPBAAgaQg6HktL8avs82P1sWCKOrpOaW9zu9dNAgBg0CDoDAABv09Tx35cktSwv9XbxgAAMIgQdAaIq8dlS5J2/oWgAwBAshB0BohrPjVCktTwl/c9bgkAAIMHQWeA+OzYjyvg9+mdyEkdeI+HfAIAkAwEnQFieFqKCsaeHtW55/FXWEAQAIAk+MgHnfvvv1/jx4/XsGHDVFBQoD//+c9eN+lD+9H/ydfwtIC2/O97uuWBLVq76XWd7Il63SwAAD6yPtJB5/e//70qKir03e9+Vy+//LK+8IUv6MYbb9SBAwe8btqHMiEvU2v+/ir5fdIrhyL6tz+9pbsfe0XGsLYOAAAfhs98hL9FCwsL9bnPfU4PPPCAvW3SpEm6+eabVVlZecn3t7e3y7IsRSIRZWVlOdnUfvnLu8f1wptH9cP/2qNTMaPr/jpHnxs3QqOsYfrc2BH2M7IAABiK+vP9neJSm5Kuu7tbDQ0NuueeexK2FxcXa8uWLR61Kjk+lZOhT+VkKC3g1z2Pv6r6t95V/Vvv2vtDWcMU8PuUEQxoZEZQ2R9LkzFGXT0xDUsNKD0toNSATye6o/JJSg34lZbi16moUXc0poDfp9SAX36f1HUqJmOkFL9PKQGforEzNb7Tfw74ffLJJ59P8kkykoyRYsbY/y0ZxWKSkZExp2tiZ/KzW8cxMoqZs8exj20Sa+LH8ftOvz7scfw+n1L8p9vzQcc5/d6zxzn/fD/oOH6fT+acuoT32G05286Ltcfn853T/sRzO7evY2f2+3TmOAGfAh4e59y/+9Pn5VfAf7pv4scxxtg/7+zfz9m+Tjy+FPDrA48T/3kJf8/ntPPs32Xyj5PiT+yfD/q7v1AfGXPp9vT1OH6f7N+xvh7nQr8ffTlOX37PPuxxzm+nz6c+/W5c6vf1co5zbjvjx/H7e/9u9Pczfe7vmN93+hhxff19TcZxzv8sxj/T/jPHufpTI1RyVVhe+cgGnXfffVfRaFR5eXkJ2/Py8tTc3HzB93R1damrq8v+c3v7wF6F+NbPj9Xk0Za2/u97+t+jHTrU2qntb7+v5vaT51R1eNY+AAAupTsaI+hcjnOTp3Q6/Z6/La6yslL33nuvG81KmivDlq4MW/af20/26K2WDvl9PnWcPKX3jnfpvY5upQR8Sgv4dbInqhM9UZ2KGg1PC0g6PWrTfSqm1IDv9MhOzOhU1ChmjIIpAfl8UjRm1HNmJCc1xa+YMYpGjU7FTv+LRmf+ZRP/l8fpf4XEU/vZv4d4io//DUQvcJzYmX/V+Ox/pZ37L9HE4/vPHChmpGgspp5oco5zKnb6/f09zrkjN/E+63WcMx3hP+89fv/ZPopvNxdrTz+PEzOJ7Tn3X8Y6cz6J/xLtfW7x45yKGkXP/DPSyeNcqq91pp+jsdOfITeOE/+8xI+jc873Ysc5/zN0qePEf8d6+nCcD/o9u+Rx+vj7eu5xTsVMwqjJBx3nUp/pnujZ48SMOfu+Pv6d9ec45/f1B34WY7FeI6rnn9vlHOdCv6/nHyve15c6zqV+zxKOc+Z3LHa5xzmvPfb/O/vwd2Z/hmJGp6KJx5ky+uPy0kc26OTk5CgQCPQavWlpaek1yhO3YsUK3Xnnnfaf29vbNWbMGEfbmWxZw1L1uTO3oQMAgIv7yN51lZaWpoKCAtXV1SVsr6ur0/Tp0y/4nmAwqKysrIQXAAAYvD6yIzqSdOedd6q8vFxXX321ioqK9NBDD+nAgQO6/fbbvW4aAAAYAD7SQeerX/2q3nvvPf3whz9UU1OT8vPz9cwzz2jcuHFeNw0AAAwAH+l1dC7XQF1HBwAAfLD+fH9/ZOfoAAAAXApBBwAADFoEHQAAMGgRdAAAwKBF0AEAAIMWQQcAAAxaBB0AADBoEXQAAMCgRdABAACD1kf6ERCXK74odHt7u8ctAQAAfRX/3u7Lwx2GdNA5duyYJGnMmDEetwQAAPTXsWPHZFnWRWuG9LOuYrGY3nnnHWVmZsrn8yX12O3t7RozZowOHjzIc7Qugb7qO/qqf+ivvqOv+of+6jsn+soYo2PHjikcDsvvv/gsnCE9ouP3+zV69GhHf0ZWVha/BH1EX/UdfdU/9Fff0Vf9Q3/1XbL76lIjOXFMRgYAAIMWQQcAAAxaBB2HBINB/eAHP1AwGPS6KQMefdV39FX/0F99R1/1D/3Vd1731ZCejAwAAAY3RnQAAMCgRdABAACDFkEHAAAMWgQdAAAwaBF0HHD//fdr/PjxGjZsmAoKCvTnP//Z6yZ5buXKlfL5fAmvUChk7zfGaOXKlQqHw0pPT9eMGTO0e/duD1vsrhdeeEFz5sxROByWz+fTE088kbC/L/3T1dWlJUuWKCcnRxkZGSotLdWhQ4dcPAt3XKqvvvnNb/b6rE2bNi2hZqj0VWVlpa655hplZmYqNzdXN998s15//fWEGj5bp/Wlr/hsnfXAAw/oqquushcBLCoq0h/+8Ad7/0D6XBF0kuz3v/+9Kioq9N3vflcvv/yyvvCFL+jGG2/UgQMHvG6a56688ko1NTXZr1dffdXet2bNGq1du1ZVVVXauXOnQqGQbrjhBvt5ZIPd8ePHNWXKFFVVVV1wf1/6p6KiQjU1NaqurlZ9fb06OjpUUlKiaDTq1mm44lJ9JUmzZ89O+Kw988wzCfuHSl9t3rxZd9xxh7Zt26a6ujqdOnVKxcXFOn78uF3DZ+u0vvSVxGcrbvTo0Vq9erVefPFFvfjii/rSl76kr3zlK3aYGVCfK4Ok+vznP29uv/32hG2f+cxnzD333ONRiwaGH/zgB2bKlCkX3BeLxUwoFDKrV6+2t508edJYlmUefPBBl1o4cEgyNTU19p/70j9tbW0mNTXVVFdX2zWHDx82fr/f1NbWutZ2t53fV8YYM2/ePPOVr3zlA98zVPvKGGNaWlqMJLN582ZjDJ+tizm/r4zhs3UpI0aMML/85S8H3OeKEZ0k6u7uVkNDg4qLixO2FxcXa8uWLR61auB48803FQ6HNX78eN166616++23JUn79u1Tc3NzQr8Fg0Fdf/319Jv61j8NDQ3q6elJqAmHw8rPzx+Sffj8888rNzdXf/M3f6MFCxaopaXF3jeU+yoSiUiSsrOzJfHZupjz+yqOz1Zv0WhU1dXVOn78uIqKigbc54qgk0TvvvuuotGo8vLyErbn5eWpubnZo1YNDIWFhfrNb36jZ599Vg8//LCam5s1ffp0vffee3bf0G8X1pf+aW5uVlpamkaMGPGBNUPFjTfeqI0bN+pPf/qTfvrTn2rnzp360pe+pK6uLklDt6+MMbrzzjt13XXXKT8/XxKfrQ9yob6S+Gyd79VXX9XHPvYxBYNB3X777aqpqdEVV1wx4D5XQ/rp5U7x+XwJfzbG9No21Nx44432f0+ePFlFRUX6q7/6Kz3yyCP2ZD767eI+TP8MxT786le/av93fn6+rr76ao0bN05PP/20brnllg9832Dvq8WLF+uVV15RfX19r318thJ9UF/x2Uo0ceJENTY2qq2tTY899pjmzZunzZs32/sHyueKEZ0kysnJUSAQ6JVGW1paeiXboS4jI0OTJ0/Wm2++ad99Rb9dWF/6JxQKqbu7W62trR9YM1SNGjVK48aN05tvvilpaPbVkiVL9OSTT+q5557T6NGj7e18tnr7oL66kKH+2UpLS9Nf//Vf6+qrr1ZlZaWmTJmin//85wPuc0XQSaK0tDQVFBSorq4uYXtdXZ2mT5/uUasGpq6uLr322msaNWqUxo8fr1AolNBv3d3d2rx5M/0m9al/CgoKlJqamlDT1NSkXbt2Dfk+fO+993Tw4EGNGjVK0tDqK2OMFi9erMcff1x/+tOfNH78+IT9fLbOulRfXchQ/mxdiDFGXV1dA+9zldSpzTDV1dUmNTXV/OpXvzJ79uwxFRUVJiMjw/zlL3/xummeWrZsmXn++efN22+/bbZt22ZKSkpMZmam3S+rV682lmWZxx9/3Lz66qvma1/7mhk1apRpb2/3uOXuOHbsmHn55ZfNyy+/bCSZtWvXmpdfftns37/fGNO3/rn99tvN6NGjzR//+Efz0ksvmS996UtmypQp5tSpU16dliMu1lfHjh0zy5YtM1u2bDH79u0zzz33nCkqKjKf/OQnh2Rf/eM//qOxLMs8//zzpqmpyX6dOHHCruGzddql+orPVqIVK1aYF154wezbt8+88sor5l/+5V+M3+83mzZtMsYMrM8VQccB//7v/27GjRtn0tLSzOc+97mE2xOHqq9+9atm1KhRJjU11YTDYXPLLbeY3bt32/tjsZj5wQ9+YEKhkAkGg+Zv//Zvzauvvuphi9313HPPGUm9XvPmzTPG9K1/Ojs7zeLFi012drZJT083JSUl5sCBAx6cjbMu1lcnTpwwxcXF5hOf+IRJTU01Y8eONfPmzevVD0Olry7UT5LMr3/9a7uGz9Zpl+orPluJvvWtb9nfc5/4xCfMzJkz7ZBjzMD6XPmMMSa5Y0QAAAADA3N0AADAoEXQAQAAgxZBBwAADFoEHQAAMGgRdAAAwKBF0AEAAIMWQQcAAAxaBB0AADBoEXQAAMCgRdABAACDFkEHAAAMWgQdAAAwaP3/2LTsVQh2BcUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a35600-5872-421e-9eaf-8ad62551acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOMENTUM_BGD_LR(x,y,alpha=0.05, momentum_gama=0,itirations=1000,gradient_norm=0.001,loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    m=x.shape[0]\n",
    "    loss=[]\n",
    "    x=np.array(x)\n",
    "    y=np.array(y).reshape(-1,1)\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    w=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    thetas=[] # return tensor \n",
    "    momentam=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    for i in range (itirations):\n",
    "        hx=x@w\n",
    "        e=hx-y\n",
    "        j=e.T@e/(2*m)\n",
    "        loss.append(j)\n",
    "        G=(x.T@e)/m\n",
    "        vt=alpha*G+momentum_gama*momentam\n",
    "        thetas.append(w.copy())\n",
    "        if np.linalg.norm(G)<=(gradient_norm):\n",
    "            print (f''' Gradient Norm condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}''')\n",
    "            break\n",
    "        if i>2 and np.absolute(loss[i-1]-loss[i])<loss_condition:\n",
    "            print (f'''loss condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={loss[i]-loss[i-1]}\n",
    "            ''')\n",
    "            break\n",
    "        \n",
    "        w=w-vt\n",
    "        momentam=vt\n",
    "    hx=x@w\n",
    "    print('you finshed your itirations ')\n",
    "    print (f'R2 score ={r2_score(y,hx)}')\n",
    "    return np.array (w) ,hx,np.array(loss),np.array(thetas)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19b14daa-a2f5-4ea3-9e72-ebd760367a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOMENTUM_BGD_LR(x, y, alpha=0.05, momentum_gama=0.9, itirations=1000, gradient_norm=0.001, loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. Prepare data and get dimensions\n",
    "    m = x.shape[0]\n",
    "    loss = []\n",
    "    x = np.array(x)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "    # 2. Add bias term (column of ones) to features matrix\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "\n",
    "    # 3. Initialize weights and velocity (momentum) vector\n",
    "    w = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    thetas = [] \n",
    "    momentam = np.zeros(x.shape[1]).reshape(-1, 1) # This stores the moving average of gradients\n",
    "\n",
    "    for i in range(itirations):\n",
    "        # 4. Standard Forward Pass: prediction using current weights\n",
    "        hx = x @ w\n",
    "        \n",
    "        # 5. Calculate Error and Cost (MSE)\n",
    "        e = hx - y\n",
    "        j = (e.T @ e) / (2 * m)\n",
    "        loss.append(j)\n",
    "        \n",
    "        # 6. Compute Gradient at the current position\n",
    "        G = (x.T @ e) / m\n",
    "        \n",
    "        # 7. Update Velocity (vt): \n",
    "        # Combined weighted average of previous velocity and current gradient\n",
    "        vt = alpha * G + momentum_gama * momentam\n",
    "        \n",
    "        thetas.append(w.copy())\n",
    "        \n",
    "        # 8. Convergence Check: Stop if gradient is small enough\n",
    "        if np.linalg.norm(G) <= (gradient_norm):\n",
    "            print(f'Gradient Norm condition - Stopped at epoch {i}')\n",
    "            break\n",
    "            \n",
    "        # 9. Convergence Check: Stop if loss improvement is negligible\n",
    "        if i > 2 and np.absolute(loss[i-1] - loss[i]) < loss_condition:\n",
    "            print(f'Loss condition - Stopped at epoch {i}')\n",
    "            break\n",
    "        \n",
    "        # 10. Parameter Update: Update weights using the velocity\n",
    "        w = w - vt\n",
    "        \n",
    "        # 11. Store current velocity for the next iteration\n",
    "        momentam = vt\n",
    "\n",
    "    # 12. Evaluation\n",
    "    hx_final = x @ w\n",
    "    print('Training Finished.')\n",
    "    print(f'Final R2 score = {r2_score(y, hx_final)}')\n",
    "    \n",
    "    return np.array(w), hx_final, np.array(loss), np.array(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb53fccd-2744-481c-88af-1d517b98386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBGD_LR(x,y,  alpha=0.05,batch_size=100 , momentem_gama=0,epochs=10 ,  gradient_norm=0.001  ,  loss_condition=0.001):\n",
    "    m=x.shape[0]\n",
    "    x=np.array(x)\n",
    "    y=np.array(y).reshape(-1,1)\n",
    "    all_data=np.concatenate((x,y),axis=1)\n",
    "    np.random.shuffle(all_data)\n",
    "    x=all_data[:,:-1]\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    y=all_data[:,-1].reshape(-1,1)\n",
    "    m=x.shape[0]\n",
    "    loss=[]\n",
    "    w=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    thetas=[] # return tensor \n",
    "    total_loss=[]\n",
    "    momentam=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    for i in range (epochs) :\n",
    "        start=0 \n",
    "        end= 0\n",
    "        total_epoch_loss=0\n",
    "        while(start<m):\n",
    "            end=start+batch_size if (start+batch_size)<m else m\n",
    "            x_temp=x[start:end,:]\n",
    "            y_temp=y[start:end,:]\n",
    "            m_temp=x_temp.shape[0]\n",
    "            hx=x_temp@w\n",
    "            e=hx-y_temp\n",
    "            j=e.T@e/(2*m_temp)\n",
    "            loss.append(j)\n",
    "            G=(x_temp.T@e)/m_temp\n",
    "            vt=alpha*G+momentam_gama*momentam\n",
    "            thetas.append(w.copy())\n",
    "            start=end\n",
    "            w=w-vt\n",
    "            momentam=vt\n",
    "            total_epoch_loss+=np.absolute(j)\n",
    "        total_loss.append(total_epoch_loss)\n",
    "        if np.linalg.norm(G)<=(gradient_norm):\n",
    "             print (f''' Gradient Norm condation\n",
    "             stop in epoch number {i} \n",
    "             gredient norm = {np.linalg.norm(G)}''')\n",
    "             break\n",
    "        if i>2 and np.absolute(total_loss[i-1]-total_loss[i])<loss_condition:\n",
    "            print (f'''loss condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={total_loss[i-1]-total_loss[i]}\n",
    "            ''')\n",
    "            break\n",
    "    y_mean = np.mean(y)\n",
    "    ss_res, ss_tot = 0, 0\n",
    "    for s in range(0, m, batch_size):\n",
    "        e = min(s + batch_size, m)\n",
    "        y_p = x[s:e, :] @ w\n",
    "        ss_res += np.sum((y[s:e] - y_p)**2)\n",
    "        ss_tot += np.sum((y[s:e] - y_mean)**2)\n",
    "    \n",
    "    print(f'Final R2 Score = {1 - (ss_res / ss_tot)}') \n",
    "    print('you finshed your itirations ')\n",
    "    \n",
    "    return np.array (w) ,np.array(loss),np.array(thetas)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74d3b898-f7cd-4dba-9fbc-9a0c7a82ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAG_MBGD_LR(x,y,  alpha=0.05,batch_size=100 , momentam_gama=0,epochs=10 ,  gradient_norm=0.001  ,  loss_condition=0.001):\n",
    "    m=x.shape[0]\n",
    "    x=np.array(x)\n",
    "    y=np.array(y).reshape(-1,1)\n",
    "    all_data=np.concatenate((x,y),axis=1)\n",
    "    np.random.shuffle(all_data)\n",
    "    x=all_data[:,:-1]\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    y=all_data[:,-1].reshape(-1,1)\n",
    "    m=x.shape[0]\n",
    "    loss=[]\n",
    "    w=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    w_temp=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    thetas=[] # return tensor \n",
    "    total_loss=[]\n",
    "    momentam=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    for i in range (epochs) :\n",
    "        start=0 \n",
    "        end= 0\n",
    "        total_epoch_loss=0\n",
    "        \n",
    "        while(start<m):\n",
    "            end=start+batch_size if (start+batch_size)<m else m\n",
    "            x_temp=x[start:end,:]\n",
    "            y_temp=y[start:end,:]\n",
    "            m_temp=x_temp.shape[0]\n",
    "            w_temp=w-momentam_gama*momentam\n",
    "            hx=x_temp@w_temp\n",
    "            e=hx-y_temp\n",
    "            j=e.T@e/(2*m_temp)\n",
    "            loss.append(j)\n",
    "            G=(x_temp.T@e)/m_temp\n",
    "            vt=alpha*G+momentam_gama*momentam\n",
    "            thetas.append(w.copy())\n",
    "            start=end\n",
    "            w=w-vt\n",
    "            momentam=vt\n",
    "            total_epoch_loss+=np.absolute(j)\n",
    "        total_loss.append(total_epoch_loss)\n",
    "        if np.linalg.norm(G)<=(gradient_norm):\n",
    "             print (f''' Gradient Norm condation\n",
    "             stop in epoch number {i} \n",
    "             gredient norm = {np.linalg.norm(G)}''')\n",
    "             break\n",
    "        if i>2 and np.absolute(total_loss[i-1]-total_loss[i])<loss_condition:\n",
    "            print (f'''loss condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={total_loss[i-1]-total_loss[i]}\n",
    "            ''')\n",
    "            break\n",
    "    y_mean = np.mean(y)\n",
    "    ss_res, ss_tot = 0, 0\n",
    "    for s in range(0, m, batch_size):\n",
    "        e = min(s + batch_size, m)\n",
    "        y_p = x[s:e, :] @ w\n",
    "        ss_res += np.sum((y[s:e] - y_p)**2)\n",
    "        ss_tot += np.sum((y[s:e] - y_mean)**2)\n",
    "    \n",
    "    print(f'Final R2 Score = {1 - (ss_res / ss_tot)}') \n",
    "    print('you finshed your itirations ')\n",
    "    \n",
    "    return np.array (w) ,np.array(loss),np.array(thetas)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56644e9-6663-416f-834d-62eb37da290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAG_BGD_LR(x,y,alpha=0.05, momentum_gama=0,itirations=1000,gradient_norm=0.001,loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    m=x.shape[0]\n",
    "    loss=[]\n",
    "    x=np.array(x)\n",
    "    y=np.array(y).reshape(-1,1)\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    w=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    thetas=[] # return tensor \n",
    "    momentam=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    temp=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    for i in range (itirations):\n",
    "        temp=w-momentum_gama*momentam\n",
    "        hx=x@temp\n",
    "        e=hx-y\n",
    "        j=e.T@e/(2*m)\n",
    "        loss.append(j)\n",
    "        G=(x.T@e)/m\n",
    "        vt=alpha*G+momentum_gama*momentam\n",
    "        thetas.append(w.copy())\n",
    "        if np.linalg.norm(G)<=(gradient_norm):\n",
    "            print (f''' Gradient Norm condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}''')\n",
    "            break\n",
    "        if i>2 and np.absolute(loss[i-1]-loss[i])<loss_condition:\n",
    "            print (f'''loss condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={loss[i]-loss[i-1]}\n",
    "            ''')\n",
    "            break\n",
    "        \n",
    "        w=w-vt\n",
    "        momentam=vt\n",
    "    hx=x@w\n",
    "    print('you finshed your itirations ')\n",
    "    print (f'R2 score ={r2_score(y,hx)}')\n",
    "    return np.array (w) ,hx,np.array(loss),np.array(thetas)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9003c485-d2dd-4be8-bacc-ad803448846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAG_BGD_LR(x, y, alpha=0.05, momentum_gama=0.9, itirations=1000, gradient_norm=0.001, loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. Initialize constants and convert inputs to numpy arrays\n",
    "    m = x.shape[0]\n",
    "    loss = []\n",
    "    x = np.array(x)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "    # 2. Add intercept term (bias) to the features matrix\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "\n",
    "    # 3. Initialize parameters (Weights, Velocities, and Temporary Look-ahead theta)\n",
    "    w = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    thetas = [] # Track weight history\n",
    "    momentam = np.zeros(x.shape[1]).reshape(-1, 1) # Velocity storage\n",
    "\n",
    "    for i in range(itirations):\n",
    "        # 4. Nesterov Look-ahead Step: jump forward using previous velocity\n",
    "        # This is where we \"peek\" at the next position before calculating gradient\n",
    "        temp = w - momentum_gama * momentam\n",
    "        \n",
    "        # 5. Prediction and Loss calculation at the Look-ahead point\n",
    "        hx = x @ temp\n",
    "        e = hx - y\n",
    "        j = (e.T @ e) / (2 * m)\n",
    "        loss.append(j)\n",
    "        \n",
    "        # 6. Gradient calculation at the Look-ahead point (NAG core logic)\n",
    "        G = (x.T @ e) / m\n",
    "        \n",
    "        # 7. Update Velocity (Integrated Gradient and Momentum)\n",
    "        vt = alpha * G + momentum_gama * momentam\n",
    "        \n",
    "        thetas.append(w.copy())\n",
    "        \n",
    "        # 8. Check Convergence: Gradient Norm Condition\n",
    "        if np.linalg.norm(G) <= (gradient_norm):\n",
    "            print(f'Gradient Norm condition - Stopped at epoch {i}')\n",
    "            break\n",
    "            \n",
    "        # 9. Check Convergence: Change in Loss Condition\n",
    "        if i > 2 and np.absolute(loss[i-1] - loss[i]) < loss_condition:\n",
    "            print(f'Loss condition - Stopped at epoch {i}')\n",
    "            break\n",
    "        \n",
    "        # 10. Final Parameter Update: Step in the direction of the updated velocity\n",
    "        w = w - vt\n",
    "        \n",
    "        # 11. Store velocity for the next look-ahead jump\n",
    "        momentam = vt\n",
    "\n",
    "    # 12. Final evaluation\n",
    "    hx_final = x @ w\n",
    "    print('Training Finished.')\n",
    "    print(f'Final R2 score = {r2_score(y, hx_final)}')\n",
    "    \n",
    "    return np.array(w), hx_final, np.array(loss), np.array(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2549bc0a-3a3c-4050-a420-dc65bc65b828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss condition - Stopped at epoch 337\n",
      "Training Finished.\n",
      "Final R2 score = 0.9788689636596116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.00825279],\n",
       "        [0.665216  ],\n",
       "        [0.66869265],\n",
       "        [0.68873407]]),\n",
       " array([[153.71948879],\n",
       "        [184.77056338],\n",
       "        [182.04957512],\n",
       "        [198.27427645],\n",
       "        [140.91412126],\n",
       "        [103.90493703],\n",
       "        [148.423937  ],\n",
       "        [110.04423793],\n",
       "        [172.69483126],\n",
       "        [159.97740121],\n",
       "        [142.99423009],\n",
       "        [141.00471689],\n",
       "        [188.07394382],\n",
       "        [156.33331665],\n",
       "        [149.10919442],\n",
       "        [187.50545826],\n",
       "        [148.88096701],\n",
       "        [178.12535667],\n",
       "        [179.29811548],\n",
       "        [160.42911491],\n",
       "        [174.04959983],\n",
       "        [173.34695915],\n",
       "        [165.9389875 ],\n",
       "        [154.96627846],\n",
       "        [191.48714281]]),\n",
       " array([[[1.34059800e+04]],\n",
       " \n",
       "        [[1.28859125e+04]],\n",
       " \n",
       "        [[1.23860292e+04]],\n",
       " \n",
       "        [[1.19055467e+04]],\n",
       " \n",
       "        [[1.14437120e+04]],\n",
       " \n",
       "        [[1.09998015e+04]],\n",
       " \n",
       "        [[1.05731195e+04]],\n",
       " \n",
       "        [[1.01629973e+04]],\n",
       " \n",
       "        [[9.76879218e+03]],\n",
       " \n",
       "        [[9.38988648e+03]],\n",
       " \n",
       "        [[9.02568636e+03]],\n",
       " \n",
       "        [[8.67562111e+03]],\n",
       " \n",
       "        [[8.33914213e+03]],\n",
       " \n",
       "        [[8.01572214e+03]],\n",
       " \n",
       "        [[7.70485430e+03]],\n",
       " \n",
       "        [[7.40605145e+03]],\n",
       " \n",
       "        [[7.11884536e+03]],\n",
       " \n",
       "        [[6.84278593e+03]],\n",
       " \n",
       "        [[6.57744055e+03]],\n",
       " \n",
       "        [[6.32239342e+03]],\n",
       " \n",
       "        [[6.07724484e+03]],\n",
       " \n",
       "        [[5.84161064e+03]],\n",
       " \n",
       "        [[5.61512158e+03]],\n",
       " \n",
       "        [[5.39742270e+03]],\n",
       " \n",
       "        [[5.18817287e+03]],\n",
       " \n",
       "        [[4.98704418e+03]],\n",
       " \n",
       "        [[4.79372142e+03]],\n",
       " \n",
       "        [[4.60790165e+03]],\n",
       " \n",
       "        [[4.42929368e+03]],\n",
       " \n",
       "        [[4.25761760e+03]],\n",
       " \n",
       "        [[4.09260439e+03]],\n",
       " \n",
       "        [[3.93399545e+03]],\n",
       " \n",
       "        [[3.78154223e+03]],\n",
       " \n",
       "        [[3.63500583e+03]],\n",
       " \n",
       "        [[3.49415661e+03]],\n",
       " \n",
       "        [[3.35877383e+03]],\n",
       " \n",
       "        [[3.22864536e+03]],\n",
       " \n",
       "        [[3.10356726e+03]],\n",
       " \n",
       "        [[2.98334352e+03]],\n",
       " \n",
       "        [[2.86778575e+03]],\n",
       " \n",
       "        [[2.75671285e+03]],\n",
       " \n",
       "        [[2.64995077e+03]],\n",
       " \n",
       "        [[2.54733219e+03]],\n",
       " \n",
       "        [[2.44869631e+03]],\n",
       " \n",
       "        [[2.35388856e+03]],\n",
       " \n",
       "        [[2.26276035e+03]],\n",
       " \n",
       "        [[2.17516889e+03]],\n",
       " \n",
       "        [[2.09097692e+03]],\n",
       " \n",
       "        [[2.01005249e+03]],\n",
       " \n",
       "        [[1.93226878e+03]],\n",
       " \n",
       "        [[1.85750392e+03]],\n",
       " \n",
       "        [[1.78564073e+03]],\n",
       " \n",
       "        [[1.71656660e+03]],\n",
       " \n",
       "        [[1.65017327e+03]],\n",
       " \n",
       "        [[1.58635672e+03]],\n",
       " \n",
       "        [[1.52501692e+03]],\n",
       " \n",
       "        [[1.46605777e+03]],\n",
       " \n",
       "        [[1.40938685e+03]],\n",
       " \n",
       "        [[1.35491537e+03]],\n",
       " \n",
       "        [[1.30255796e+03]],\n",
       " \n",
       "        [[1.25223258e+03]],\n",
       " \n",
       "        [[1.20386035e+03]],\n",
       " \n",
       "        [[1.15736549e+03]],\n",
       " \n",
       "        [[1.11267511e+03]],\n",
       " \n",
       "        [[1.06971920e+03]],\n",
       " \n",
       "        [[1.02843043e+03]],\n",
       " \n",
       "        [[9.88744103e+02]],\n",
       " \n",
       "        [[9.50598027e+02]],\n",
       " \n",
       "        [[9.13932423e+02]],\n",
       " \n",
       "        [[8.78689832e+02]],\n",
       " \n",
       "        [[8.44815027e+02]],\n",
       " \n",
       "        [[8.12254923e+02]],\n",
       " \n",
       "        [[7.80958496e+02]],\n",
       " \n",
       "        [[7.50876700e+02]],\n",
       " \n",
       "        [[7.21962397e+02]],\n",
       " \n",
       "        [[6.94170273e+02]],\n",
       " \n",
       "        [[6.67456777e+02]],\n",
       " \n",
       "        [[6.41780047e+02]],\n",
       " \n",
       "        [[6.17099845e+02]],\n",
       " \n",
       "        [[5.93377494e+02]],\n",
       " \n",
       "        [[5.70575820e+02]],\n",
       " \n",
       "        [[5.48659091e+02]],\n",
       " \n",
       "        [[5.27592962e+02]],\n",
       " \n",
       "        [[5.07344420e+02]],\n",
       " \n",
       "        [[4.87881734e+02]],\n",
       " \n",
       "        [[4.69174404e+02]],\n",
       " \n",
       "        [[4.51193114e+02]],\n",
       " \n",
       "        [[4.33909687e+02]],\n",
       " \n",
       "        [[4.17297038e+02]],\n",
       " \n",
       "        [[4.01329133e+02]],\n",
       " \n",
       "        [[3.85980950e+02]],\n",
       " \n",
       "        [[3.71228436e+02]],\n",
       " \n",
       "        [[3.57048473e+02]],\n",
       " \n",
       "        [[3.43418841e+02]],\n",
       " \n",
       "        [[3.30318180e+02]],\n",
       " \n",
       "        [[3.17725960e+02]],\n",
       " \n",
       "        [[3.05622450e+02]],\n",
       " \n",
       "        [[2.93988680e+02]],\n",
       " \n",
       "        [[2.82806422e+02]],\n",
       " \n",
       "        [[2.72058150e+02]],\n",
       " \n",
       "        [[2.61727023e+02]],\n",
       " \n",
       "        [[2.51796849e+02]],\n",
       " \n",
       "        [[2.42252068e+02]],\n",
       " \n",
       "        [[2.33077723e+02]],\n",
       " \n",
       "        [[2.24259436e+02]],\n",
       " \n",
       "        [[2.15783389e+02]],\n",
       " \n",
       "        [[2.07636299e+02]],\n",
       " \n",
       "        [[1.99805400e+02]],\n",
       " \n",
       "        [[1.92278418e+02]],\n",
       " \n",
       "        [[1.85043560e+02]],\n",
       " \n",
       "        [[1.78089488e+02]],\n",
       " \n",
       "        [[1.71405304e+02]],\n",
       " \n",
       "        [[1.64980533e+02]],\n",
       " \n",
       "        [[1.58805108e+02]],\n",
       " \n",
       "        [[1.52869352e+02]],\n",
       " \n",
       "        [[1.47163962e+02]],\n",
       " \n",
       "        [[1.41679998e+02]],\n",
       " \n",
       "        [[1.36408867e+02]],\n",
       " \n",
       "        [[1.31342307e+02]],\n",
       " \n",
       "        [[1.26472380e+02]],\n",
       " \n",
       "        [[1.21791454e+02]],\n",
       " \n",
       "        [[1.17292194e+02]],\n",
       " \n",
       "        [[1.12967549e+02]],\n",
       " \n",
       "        [[1.08810742e+02]],\n",
       " \n",
       "        [[1.04815260e+02]],\n",
       " \n",
       "        [[1.00974841e+02]],\n",
       " \n",
       "        [[9.72834673e+01]],\n",
       " \n",
       "        [[9.37353539e+01]],\n",
       " \n",
       "        [[9.03249412e+01]],\n",
       " \n",
       "        [[8.70468847e+01]],\n",
       " \n",
       "        [[8.38960477e+01]],\n",
       " \n",
       "        [[8.08674926e+01]],\n",
       " \n",
       "        [[7.79564736e+01]],\n",
       " \n",
       "        [[7.51584291e+01]],\n",
       " \n",
       "        [[7.24689745e+01]],\n",
       " \n",
       "        [[6.98838952e+01]],\n",
       " \n",
       "        [[6.73991405e+01]],\n",
       " \n",
       "        [[6.50108166e+01]],\n",
       " \n",
       "        [[6.27151810e+01]],\n",
       " \n",
       "        [[6.05086363e+01]],\n",
       " \n",
       "        [[5.83877250e+01]],\n",
       " \n",
       "        [[5.63491235e+01]],\n",
       " \n",
       "        [[5.43896374e+01]],\n",
       " \n",
       "        [[5.25061960e+01]],\n",
       " \n",
       "        [[5.06958481e+01]],\n",
       " \n",
       "        [[4.89557568e+01]],\n",
       " \n",
       "        [[4.72831955e+01]],\n",
       " \n",
       "        [[4.56755432e+01]],\n",
       " \n",
       "        [[4.41302808e+01]],\n",
       " \n",
       "        [[4.26449868e+01]],\n",
       " \n",
       "        [[4.12173340e+01]],\n",
       " \n",
       "        [[3.98450851e+01]],\n",
       " \n",
       "        [[3.85260899e+01]],\n",
       " \n",
       "        [[3.72582816e+01]],\n",
       " \n",
       "        [[3.60396736e+01]],\n",
       " \n",
       "        [[3.48683564e+01]],\n",
       " \n",
       "        [[3.37424946e+01]],\n",
       " \n",
       "        [[3.26603240e+01]],\n",
       " \n",
       "        [[3.16201490e+01]],\n",
       " \n",
       "        [[3.06203397e+01]],\n",
       " \n",
       "        [[2.96593295e+01]],\n",
       " \n",
       "        [[2.87356125e+01]],\n",
       " \n",
       "        [[2.78477414e+01]],\n",
       " \n",
       "        [[2.69943249e+01]],\n",
       " \n",
       "        [[2.61740259e+01]],\n",
       " \n",
       "        [[2.53855591e+01]],\n",
       " \n",
       "        [[2.46276889e+01]],\n",
       " \n",
       "        [[2.38992280e+01]],\n",
       " \n",
       "        [[2.31990349e+01]],\n",
       " \n",
       "        [[2.25260126e+01]],\n",
       " \n",
       "        [[2.18791065e+01]],\n",
       " \n",
       "        [[2.12573031e+01]],\n",
       " \n",
       "        [[2.06596280e+01]],\n",
       " \n",
       "        [[2.00851449e+01]],\n",
       " \n",
       "        [[1.95329537e+01]],\n",
       " \n",
       "        [[1.90021892e+01]],\n",
       " \n",
       "        [[1.84920197e+01]],\n",
       " \n",
       "        [[1.80016461e+01]],\n",
       " \n",
       "        [[1.75302999e+01]],\n",
       " \n",
       "        [[1.70772428e+01]],\n",
       " \n",
       "        [[1.66417650e+01]],\n",
       " \n",
       "        [[1.62231841e+01]],\n",
       " \n",
       "        [[1.58208444e+01]],\n",
       " \n",
       "        [[1.54341155e+01]],\n",
       " \n",
       "        [[1.50623916e+01]],\n",
       " \n",
       "        [[1.47050903e+01]],\n",
       " \n",
       "        [[1.43616520e+01]],\n",
       " \n",
       "        [[1.40315384e+01]],\n",
       " \n",
       "        [[1.37142326e+01]],\n",
       " \n",
       "        [[1.34092374e+01]],\n",
       " \n",
       "        [[1.31160751e+01]],\n",
       " \n",
       "        [[1.28342863e+01]],\n",
       " \n",
       "        [[1.25634297e+01]],\n",
       " \n",
       "        [[1.23030811e+01]],\n",
       " \n",
       "        [[1.20528325e+01]],\n",
       " \n",
       "        [[1.18122920e+01]],\n",
       " \n",
       "        [[1.15810828e+01]],\n",
       " \n",
       "        [[1.13588427e+01]],\n",
       " \n",
       "        [[1.11452238e+01]],\n",
       " \n",
       "        [[1.09398913e+01]],\n",
       " \n",
       "        [[1.07425236e+01]],\n",
       " \n",
       "        [[1.05528117e+01]],\n",
       " \n",
       "        [[1.03704584e+01]],\n",
       " \n",
       "        [[1.01951782e+01]],\n",
       " \n",
       "        [[1.00266964e+01]],\n",
       " \n",
       "        [[9.86474935e+00]],\n",
       " \n",
       "        [[9.70908332e+00]],\n",
       " \n",
       "        [[9.55945456e+00]],\n",
       " \n",
       "        [[9.41562876e+00]],\n",
       " \n",
       "        [[9.27738072e+00]],\n",
       " \n",
       "        [[9.14449394e+00]],\n",
       " \n",
       "        [[9.01676037e+00]],\n",
       " \n",
       "        [[8.89397998e+00]],\n",
       " \n",
       "        [[8.77596057e+00]],\n",
       " \n",
       "        [[8.66251733e+00]],\n",
       " \n",
       "        [[8.55347267e+00]],\n",
       " \n",
       "        [[8.44865589e+00]],\n",
       " \n",
       "        [[8.34790288e+00]],\n",
       " \n",
       "        [[8.25105594e+00]],\n",
       " \n",
       "        [[8.15796346e+00]],\n",
       " \n",
       "        [[8.06847974e+00]],\n",
       " \n",
       "        [[7.98246471e+00]],\n",
       " \n",
       "        [[7.89978376e+00]],\n",
       " \n",
       "        [[7.82030748e+00]],\n",
       " \n",
       "        [[7.74391150e+00]],\n",
       " \n",
       "        [[7.67047627e+00]],\n",
       " \n",
       "        [[7.59988688e+00]],\n",
       " \n",
       "        [[7.53203289e+00]],\n",
       " \n",
       "        [[7.46680813e+00]],\n",
       " \n",
       "        [[7.40411056e+00]],\n",
       " \n",
       "        [[7.34384209e+00]],\n",
       " \n",
       "        [[7.28590846e+00]],\n",
       " \n",
       "        [[7.23021904e+00]],\n",
       " \n",
       "        [[7.17668674e+00]],\n",
       " \n",
       "        [[7.12522784e+00]],\n",
       " \n",
       "        [[7.07576186e+00]],\n",
       " \n",
       "        [[7.02821147e+00]],\n",
       " \n",
       "        [[6.98250231e+00]],\n",
       " \n",
       "        [[6.93856293e+00]],\n",
       " \n",
       "        [[6.89632463e+00]],\n",
       " \n",
       "        [[6.85572141e+00]],\n",
       " \n",
       "        [[6.81668980e+00]],\n",
       " \n",
       "        [[6.77916881e+00]],\n",
       " \n",
       "        [[6.74309980e+00]],\n",
       " \n",
       "        [[6.70842643e+00]],\n",
       " \n",
       "        [[6.67509453e+00]],\n",
       " \n",
       "        [[6.64305204e+00]],\n",
       " \n",
       "        [[6.61224891e+00]],\n",
       " \n",
       "        [[6.58263705e+00]],\n",
       " \n",
       "        [[6.55417022e+00]],\n",
       " \n",
       "        [[6.52680398e+00]],\n",
       " \n",
       "        [[6.50049562e+00]],\n",
       " \n",
       "        [[6.47520407e+00]],\n",
       " \n",
       "        [[6.45088989e+00]],\n",
       " \n",
       "        [[6.42751513e+00]],\n",
       " \n",
       "        [[6.40504333e+00]],\n",
       " \n",
       "        [[6.38343946e+00]],\n",
       " \n",
       "        [[6.36266982e+00]],\n",
       " \n",
       "        [[6.34270204e+00]],\n",
       " \n",
       "        [[6.32350500e+00]],\n",
       " \n",
       "        [[6.30504879e+00]],\n",
       " \n",
       "        [[6.28730465e+00]],\n",
       " \n",
       "        [[6.27024494e+00]],\n",
       " \n",
       "        [[6.25384312e+00]],\n",
       " \n",
       "        [[6.23807363e+00]],\n",
       " \n",
       "        [[6.22291195e+00]],\n",
       " \n",
       "        [[6.20833448e+00]],\n",
       " \n",
       "        [[6.19431855e+00]],\n",
       " \n",
       "        [[6.18084236e+00]],\n",
       " \n",
       "        [[6.16788497e+00]],\n",
       " \n",
       "        [[6.15542624e+00]],\n",
       " \n",
       "        [[6.14344682e+00]],\n",
       " \n",
       "        [[6.13192811e+00]],\n",
       " \n",
       "        [[6.12085223e+00]],\n",
       " \n",
       "        [[6.11020198e+00]],\n",
       " \n",
       "        [[6.09996086e+00]],\n",
       " \n",
       "        [[6.09011298e+00]],\n",
       " \n",
       "        [[6.08064308e+00]],\n",
       " \n",
       "        [[6.07153650e+00]],\n",
       " \n",
       "        [[6.06277912e+00]],\n",
       " \n",
       "        [[6.05435741e+00]],\n",
       " \n",
       "        [[6.04625832e+00]],\n",
       " \n",
       "        [[6.03846935e+00]],\n",
       " \n",
       "        [[6.03097845e+00]],\n",
       " \n",
       "        [[6.02377406e+00]],\n",
       " \n",
       "        [[6.01684505e+00]],\n",
       " \n",
       "        [[6.01018075e+00]],\n",
       " \n",
       "        [[6.00377087e+00]],\n",
       " \n",
       "        [[5.99760554e+00]],\n",
       " \n",
       "        [[5.99167527e+00]],\n",
       " \n",
       "        [[5.98597094e+00]],\n",
       " \n",
       "        [[5.98048379e+00]],\n",
       " \n",
       "        [[5.97520537e+00]],\n",
       " \n",
       "        [[5.97012759e+00]],\n",
       " \n",
       "        [[5.96524267e+00]],\n",
       " \n",
       "        [[5.96054311e+00]],\n",
       " \n",
       "        [[5.95602173e+00]],\n",
       " \n",
       "        [[5.95167161e+00]],\n",
       " \n",
       "        [[5.94748611e+00]],\n",
       " \n",
       "        [[5.94345882e+00]],\n",
       " \n",
       "        [[5.93958362e+00]],\n",
       " \n",
       "        [[5.93585461e+00]],\n",
       " \n",
       "        [[5.93226610e+00]],\n",
       " \n",
       "        [[5.92881264e+00]],\n",
       " \n",
       "        [[5.92548900e+00]],\n",
       " \n",
       "        [[5.92229014e+00]],\n",
       " \n",
       "        [[5.91921121e+00]],\n",
       " \n",
       "        [[5.91624756e+00]],\n",
       " \n",
       "        [[5.91339471e+00]],\n",
       " \n",
       "        [[5.91064837e+00]],\n",
       " \n",
       "        [[5.90800440e+00]],\n",
       " \n",
       "        [[5.90545883e+00]],\n",
       " \n",
       "        [[5.90300784e+00]],\n",
       " \n",
       "        [[5.90064775e+00]],\n",
       " \n",
       "        [[5.89837505e+00]],\n",
       " \n",
       "        [[5.89618634e+00]],\n",
       " \n",
       "        [[5.89407836e+00]],\n",
       " \n",
       "        [[5.89204797e+00]],\n",
       " \n",
       "        [[5.89009218e+00]],\n",
       " \n",
       "        [[5.88820807e+00]],\n",
       " \n",
       "        [[5.88639287e+00]],\n",
       " \n",
       "        [[5.88464391e+00]],\n",
       " \n",
       "        [[5.88295861e+00]],\n",
       " \n",
       "        [[5.88133451e+00]],\n",
       " \n",
       "        [[5.87976922e+00]],\n",
       " \n",
       "        [[5.87826047e+00]],\n",
       " \n",
       "        [[5.87680607e+00]],\n",
       " \n",
       "        [[5.87540389e+00]],\n",
       " \n",
       "        [[5.87405193e+00]],\n",
       " \n",
       "        [[5.87274822e+00]],\n",
       " \n",
       "        [[5.87149089e+00]],\n",
       " \n",
       "        [[5.87027816e+00]],\n",
       " \n",
       "        [[5.86910827e+00]],\n",
       " \n",
       "        [[5.86797958e+00]],\n",
       " \n",
       "        [[5.86689048e+00]],\n",
       " \n",
       "        [[5.86583944e+00]],\n",
       " \n",
       "        [[5.86482498e+00]],\n",
       " \n",
       "        [[5.86384568e+00]]]),\n",
       " array([[[0.00000000e+00],\n",
       "         [0.00000000e+00],\n",
       "         [0.00000000e+00],\n",
       "         [0.00000000e+00]],\n",
       " \n",
       "        [[1.62040000e-04],\n",
       "         [1.30758000e-02],\n",
       "         [1.31472000e-02],\n",
       "         [1.34670800e-02]],\n",
       " \n",
       "        [[3.20904090e-04],\n",
       "         [2.58952690e-02],\n",
       "         [2.60366584e-02],\n",
       "         [2.66704066e-02]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[8.25240592e-03],\n",
       "         [6.65189907e-01],\n",
       "         [6.68667486e-01],\n",
       "         [6.88682037e-01]],\n",
       " \n",
       "        [[8.25260155e-03],\n",
       "         [6.65203123e-01],\n",
       "         [6.68680240e-01],\n",
       "         [6.88708229e-01]],\n",
       " \n",
       "        [[8.25279299e-03],\n",
       "         [6.65216002e-01],\n",
       "         [6.68692654e-01],\n",
       "         [6.88734074e-01]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAG_BGD_LR(x,y,alpha=0.000001,momentum_gama=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb0287-94da-4439-a43d-472b4eac1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAGRAD_BGD_LR(x,y,alpha=0.05,itirations=1000,gradient_norm=0.001,loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    m=x.shape[0]\n",
    "    loss=[]\n",
    "    x=np.array(x)\n",
    "    y=np.array(y).reshape(-1,1)\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    w=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    thetas=[] # return tensor \n",
    "    adaptive=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    for i in range (itirations):\n",
    "        hx=x@w\n",
    "        e=hx-y\n",
    "        j=e.T@e/(2*m)\n",
    "        loss.append(j)\n",
    "        G=(x.T@e)/m\n",
    "         adaptive+=G**2\n",
    "        vt=(alpha/np.sqrt(adaptive+1e-8))*G\n",
    "        thetas.append(w.copy())\n",
    "        if np.linalg.norm(G)<=(gradient_norm):\n",
    "            print (f''' Gradient Norm condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}''')\n",
    "            break\n",
    "        if i>2 and np.absolute(loss[i-1]-loss[i])<loss_condition:\n",
    "            print (f'''loss condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={loss[i]-loss[i-1]}\n",
    "            ''')\n",
    "            break\n",
    "        \n",
    "        w=w-vt\n",
    "        \n",
    "    hx=x@w\n",
    "    print('you finshed your itirations ')\n",
    "    print (f'R2 score ={r2_score(y,hx)}')\n",
    "    return np.array (w) ,hx,np.array(loss),np.array(thetas)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17838a4-4da7-4a81-87c4-a8f92c389ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAGRAD_BGD_LR(x, y, alpha=0.05, itirations=1000, gradient_norm=0.001, loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    \n",
    "    # Initialization and Data Preparation\n",
    "    m = x.shape[0]\n",
    "    loss = []\n",
    "    x = np.array(x)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    \n",
    "    # Add bias term (column of ones)\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    \n",
    "    # Initialize weights and the adaptive gradient accumulator with zeros\n",
    "    w = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    thetas = [] \n",
    "    adaptive = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    \n",
    "    for i in range(itirations):\n",
    "        # Hypothesis and Error calculation\n",
    "        hx = x @ w\n",
    "        e = hx - y\n",
    "        \n",
    "        # Calculate Cost Function (Mean Squared Error)\n",
    "        j = e.T @ e / (2 * m)\n",
    "        loss.append(j)\n",
    "        \n",
    "        # Calculate Gradient (G)\n",
    "        G = (x.T @ e) / m\n",
    "        \n",
    "        # Accumulate squared gradients for adaptive learning rate\n",
    "        adaptive += G**2\n",
    "        \n",
    "        # Compute the weight update step using the Adagrad formula\n",
    "        vt = (alpha / np.sqrt(adaptive + 1e-8)) * G\n",
    "        \n",
    "        # Save current weights before update\n",
    "        thetas.append(w.copy())\n",
    "        \n",
    "        # Check convergence based on Gradient Norm\n",
    "        if np.linalg.norm(G) <= (gradient_norm):\n",
    "            print(f''' Gradient Norm condition\n",
    "            stop in epoch number {i} \n",
    "            gradient norm = {np.linalg.norm(G)}''')\n",
    "            break\n",
    "            \n",
    "        # Check convergence based on relative Loss change\n",
    "        if i > 2 and np.absolute(loss[i-1] - loss[i]) < loss_condition:\n",
    "            print(f'''loss condition\n",
    "            stop in epoch number {i} \n",
    "            gradient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={loss[i]-loss[i-1]}\n",
    "            ''')\n",
    "            break\n",
    "        \n",
    "        # Update weights\n",
    "        w = w - vt\n",
    "        \n",
    "    # Final prediction and R2 score evaluation\n",
    "    hx = x @ w\n",
    "    print('you finished your iterations')\n",
    "    print(f'R2 score ={r2_score(y, hx)}')\n",
    "    \n",
    "    return np.array(w), hx, np.array(loss), np.array(thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72d7264-5249-4d6e-afe8-616b808f3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss condition\n",
      "            stop in epoch number 109 \n",
      "            gradient norm = 16.607579076592998\n",
      "            loss[i]-loss[i-1]=[[-0.00093014]]\n",
      "            \n",
      "you finished your iterations\n",
      "R2 score =0.9783243529864961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.67142308],\n",
       "        [0.67090333],\n",
       "        [0.67073113],\n",
       "        [0.67464653]]),\n",
       " array([[153.9043463 ],\n",
       "        [184.83189941],\n",
       "        [182.13653993],\n",
       "        [198.27444643],\n",
       "        [141.14087778],\n",
       "        [104.18849063],\n",
       "        [148.54563923],\n",
       "        [110.24361464],\n",
       "        [172.74595965],\n",
       "        [159.99285977],\n",
       "        [143.16412859],\n",
       "        [141.15602277],\n",
       "        [188.1777243 ],\n",
       "        [156.58047321],\n",
       "        [149.22045795],\n",
       "        [187.52657013],\n",
       "        [149.18268158],\n",
       "        [178.12252175],\n",
       "        [179.43778163],\n",
       "        [160.62034939],\n",
       "        [174.08656096],\n",
       "        [173.4126032 ],\n",
       "        [166.01752161],\n",
       "        [155.23066358],\n",
       "        [191.54755812]]),\n",
       " array([[[1.34059800e+04]],\n",
       " \n",
       "        [[9.71707380e+03]],\n",
       " \n",
       "        [[7.64242160e+03]],\n",
       " \n",
       "        [[6.21672863e+03]],\n",
       " \n",
       "        [[5.15431612e+03]],\n",
       " \n",
       "        [[4.32679501e+03]],\n",
       " \n",
       "        [[3.66401372e+03]],\n",
       " \n",
       "        [[3.12294231e+03]],\n",
       " \n",
       "        [[2.67508092e+03]],\n",
       " \n",
       "        [[2.30049363e+03]],\n",
       " \n",
       "        [[1.98465552e+03]],\n",
       " \n",
       "        [[1.71664658e+03]],\n",
       " \n",
       "        [[1.48805090e+03]],\n",
       " \n",
       "        [[1.29225165e+03]],\n",
       " \n",
       "        [[1.12396088e+03]],\n",
       " \n",
       "        [[9.78895243e+02]],\n",
       " \n",
       "        [[8.53545851e+02]],\n",
       " \n",
       "        [[7.45010861e+02]],\n",
       " \n",
       "        [[6.50871135e+02]],\n",
       " \n",
       "        [[5.69096132e+02]],\n",
       " \n",
       "        [[4.97971505e+02]],\n",
       " \n",
       "        [[4.36042604e+02]],\n",
       " \n",
       "        [[3.82069801e+02]],\n",
       " \n",
       "        [[3.34992775e+02]],\n",
       " \n",
       "        [[2.93901656e+02]],\n",
       " \n",
       "        [[2.58013495e+02]],\n",
       " \n",
       "        [[2.26652935e+02]],\n",
       " \n",
       "        [[1.99236194e+02]],\n",
       " \n",
       "        [[1.75257731e+02]],\n",
       " \n",
       "        [[1.54279058e+02]],\n",
       " \n",
       "        [[1.35919319e+02]],\n",
       " \n",
       "        [[1.19847316e+02]],\n",
       " \n",
       "        [[1.05774729e+02]],\n",
       " \n",
       "        [[9.34503321e+01]],\n",
       " \n",
       "        [[8.26550453e+01]],\n",
       " \n",
       "        [[7.31976865e+01]],\n",
       " \n",
       "        [[6.49113154e+01]],\n",
       " \n",
       "        [[5.76500813e+01]],\n",
       " \n",
       "        [[5.12864986e+01]],\n",
       " \n",
       "        [[4.57090878e+01]],\n",
       " \n",
       "        [[4.08203313e+01]],\n",
       " \n",
       "        [[3.65348970e+01]],\n",
       " \n",
       "        [[3.27780954e+01]],\n",
       " \n",
       "        [[2.94845359e+01]],\n",
       " \n",
       "        [[2.65969572e+01]],\n",
       " \n",
       "        [[2.40652073e+01]],\n",
       " \n",
       "        [[2.18453533e+01]],\n",
       " \n",
       "        [[1.98989050e+01]],\n",
       " \n",
       "        [[1.81921364e+01]],\n",
       " \n",
       "        [[1.66954924e+01]],\n",
       " \n",
       "        [[1.53830708e+01]],\n",
       " \n",
       "        [[1.42321689e+01]],\n",
       " \n",
       "        [[1.32228865e+01]],\n",
       " \n",
       "        [[1.23377791e+01]],\n",
       " \n",
       "        [[1.15615539e+01]],\n",
       " \n",
       "        [[1.08808038e+01]],\n",
       " \n",
       "        [[1.02837741e+01]],\n",
       " \n",
       "        [[9.76015874e+00]],\n",
       " \n",
       "        [[9.30092145e+00]],\n",
       " \n",
       "        [[8.89813887e+00]],\n",
       " \n",
       "        [[8.54486345e+00]],\n",
       " \n",
       "        [[8.23500308e+00]],\n",
       " \n",
       "        [[7.96321569e+00]],\n",
       " \n",
       "        [[7.72481681e+00]],\n",
       " \n",
       "        [[7.51569853e+00]],\n",
       " \n",
       "        [[7.33225853e+00]],\n",
       " \n",
       "        [[7.17133781e+00]],\n",
       " \n",
       "        [[7.03016608e+00]],\n",
       " \n",
       "        [[6.90631392e+00]],\n",
       " \n",
       "        [[6.79765082e+00]],\n",
       " \n",
       "        [[6.70230841e+00]],\n",
       " \n",
       "        [[6.61864816e+00]],\n",
       " \n",
       "        [[6.54523312e+00]],\n",
       " \n",
       "        [[6.48080311e+00]],\n",
       " \n",
       "        [[6.42425299e+00]],\n",
       " \n",
       "        [[6.37461353e+00]],\n",
       " \n",
       "        [[6.33103474e+00]],\n",
       " \n",
       "        [[6.29277119e+00]],\n",
       " \n",
       "        [[6.25916914e+00]],\n",
       " \n",
       "        [[6.22965525e+00]],\n",
       " \n",
       "        [[6.20372671e+00]],\n",
       " \n",
       "        [[6.18094254e+00]],\n",
       " \n",
       "        [[6.16091603e+00]],\n",
       " \n",
       "        [[6.14330800e+00]],\n",
       " \n",
       "        [[6.12782102e+00]],\n",
       " \n",
       "        [[6.11419421e+00]],\n",
       " \n",
       "        [[6.10219880e+00]],\n",
       " \n",
       "        [[6.09163414e+00]],\n",
       " \n",
       "        [[6.08232428e+00]],\n",
       " \n",
       "        [[6.07411488e+00]],\n",
       " \n",
       "        [[6.06687062e+00]],\n",
       " \n",
       "        [[6.06047280e+00]],\n",
       " \n",
       "        [[6.05481731e+00]],\n",
       " \n",
       "        [[6.04981287e+00]],\n",
       " \n",
       "        [[6.04537940e+00]],\n",
       " \n",
       "        [[6.04144670e+00]],\n",
       " \n",
       "        [[6.03795317e+00]],\n",
       " \n",
       "        [[6.03484482e+00]],\n",
       " \n",
       "        [[6.03207427e+00]],\n",
       " \n",
       "        [[6.02959998e+00]],\n",
       " \n",
       "        [[6.02738552e+00]],\n",
       " \n",
       "        [[6.02539895e+00]],\n",
       " \n",
       "        [[6.02361224e+00]],\n",
       " \n",
       "        [[6.02200081e+00]],\n",
       " \n",
       "        [[6.02054312e+00]],\n",
       " \n",
       "        [[6.01922026e+00]],\n",
       " \n",
       "        [[6.01801565e+00]],\n",
       " \n",
       "        [[6.01691477e+00]],\n",
       " \n",
       "        [[6.01590484e+00]],\n",
       " \n",
       "        [[6.01497471e+00]]]),\n",
       " array([[[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       " \n",
       "        [[0.1       ],\n",
       "         [0.1       ],\n",
       "         [0.1       ],\n",
       "         [0.1       ]],\n",
       " \n",
       "        [[0.16482102],\n",
       "         [0.16482014],\n",
       "         [0.16481985],\n",
       "         [0.16482643]],\n",
       " \n",
       "        [[0.214655  ],\n",
       "         [0.21465285],\n",
       "         [0.21465214],\n",
       "         [0.21466816]],\n",
       " \n",
       "        [[0.25564654],\n",
       "         [0.25564287],\n",
       "         [0.25564165],\n",
       "         [0.25566905]],\n",
       " \n",
       "        [[0.29061118],\n",
       "         [0.29060577],\n",
       "         [0.29060397],\n",
       "         [0.29064435]],\n",
       " \n",
       "        [[0.32111529],\n",
       "         [0.32110796],\n",
       "         [0.32110552],\n",
       "         [0.32116025]],\n",
       " \n",
       "        [[0.34813758],\n",
       "         [0.34812816],\n",
       "         [0.34812502],\n",
       "         [0.34819538]],\n",
       " \n",
       "        [[0.37233923],\n",
       "         [0.37232755],\n",
       "         [0.37232367],\n",
       "         [0.37241085]],\n",
       " \n",
       "        [[0.39419277],\n",
       "         [0.39417869],\n",
       "         [0.39417401],\n",
       "         [0.39427914]],\n",
       " \n",
       "        [[0.41405071],\n",
       "         [0.41403408],\n",
       "         [0.41402855],\n",
       "         [0.41415271]],\n",
       " \n",
       "        [[0.43218497],\n",
       "         [0.43216566],\n",
       "         [0.43215924],\n",
       "         [0.43230347]],\n",
       " \n",
       "        [[0.44881114],\n",
       "         [0.44878901],\n",
       "         [0.44878165],\n",
       "         [0.44894695]],\n",
       " \n",
       "        [[0.46410399],\n",
       "         [0.4640789 ],\n",
       "         [0.46407056],\n",
       "         [0.46425791]],\n",
       " \n",
       "        [[0.47820787],\n",
       "         [0.47817971],\n",
       "         [0.47817035],\n",
       "         [0.47838069]],\n",
       " \n",
       "        [[0.49124399],\n",
       "         [0.49121264],\n",
       "         [0.49120221],\n",
       "         [0.49143644]],\n",
       " \n",
       "        [[0.50331547],\n",
       "         [0.5032808 ],\n",
       "         [0.50326927],\n",
       "         [0.50352827]],\n",
       " \n",
       "        [[0.51451109],\n",
       "         [0.51447299],\n",
       "         [0.51446032],\n",
       "         [0.51474495]],\n",
       " \n",
       "        [[0.52490811],\n",
       "         [0.52486648],\n",
       "         [0.52485263],\n",
       "         [0.52516371]],\n",
       " \n",
       "        [[0.53457434],\n",
       "         [0.53452906],\n",
       "         [0.53451401],\n",
       "         [0.53485232]],\n",
       " \n",
       "        [[0.54356976],\n",
       "         [0.54352073],\n",
       "         [0.54350443],\n",
       "         [0.54387074]],\n",
       " \n",
       "        [[0.55194779],\n",
       "         [0.55189492],\n",
       "         [0.55187735],\n",
       "         [0.55227239]],\n",
       " \n",
       "        [[0.55975632],\n",
       "         [0.55969952],\n",
       "         [0.55968064],\n",
       "         [0.56010513]],\n",
       " \n",
       "        [[0.5670385 ],\n",
       "         [0.56697767],\n",
       "         [0.56695745],\n",
       "         [0.56741206]],\n",
       " \n",
       "        [[0.57383336],\n",
       "         [0.57376841],\n",
       "         [0.57374682],\n",
       "         [0.57423222]],\n",
       " \n",
       "        [[0.58017638],\n",
       "         [0.58010724],\n",
       "         [0.58008426],\n",
       "         [0.58060106]],\n",
       " \n",
       "        [[0.58609995],\n",
       "         [0.58602653],\n",
       "         [0.58600213],\n",
       "         [0.58655093]],\n",
       " \n",
       "        [[0.59163367],\n",
       "         [0.5915559 ],\n",
       "         [0.59153005],\n",
       "         [0.59211143]],\n",
       " \n",
       "        [[0.59680472],\n",
       "         [0.59672253],\n",
       "         [0.59669521],\n",
       "         [0.59730971]],\n",
       " \n",
       "        [[0.60163811],\n",
       "         [0.60155142],\n",
       "         [0.60152261],\n",
       "         [0.60217076]],\n",
       " \n",
       "        [[0.60615688],\n",
       "         [0.60606563],\n",
       "         [0.60603531],\n",
       "         [0.6067176 ]],\n",
       " \n",
       "        [[0.61038232],\n",
       "         [0.61028645],\n",
       "         [0.61025459],\n",
       "         [0.6109715 ]],\n",
       " \n",
       "        [[0.61433413],\n",
       "         [0.61423357],\n",
       "         [0.61420015],\n",
       "         [0.61495214]],\n",
       " \n",
       "        [[0.61803055],\n",
       "         [0.61792525],\n",
       "         [0.61789026],\n",
       "         [0.61867775]],\n",
       " \n",
       "        [[0.62148851],\n",
       "         [0.62137842],\n",
       "         [0.62134184],\n",
       "         [0.62216524]],\n",
       " \n",
       "        [[0.62472375],\n",
       "         [0.62460881],\n",
       "         [0.62457062],\n",
       "         [0.62543033]],\n",
       " \n",
       "        [[0.62775087],\n",
       "         [0.62763105],\n",
       "         [0.62759123],\n",
       "         [0.6284876 ]],\n",
       " \n",
       "        [[0.6305835 ],\n",
       "         [0.63045874],\n",
       "         [0.63041728],\n",
       "         [0.63135067]],\n",
       " \n",
       "        [[0.6332343 ],\n",
       "         [0.63310455],\n",
       "         [0.63306144],\n",
       "         [0.63403218]],\n",
       " \n",
       "        [[0.63571507],\n",
       "         [0.63558031],\n",
       "         [0.63553553],\n",
       "         [0.63654392]],\n",
       " \n",
       "        [[0.63803684],\n",
       "         [0.63789701],\n",
       "         [0.63785056],\n",
       "         [0.63889691]],\n",
       " \n",
       "        [[0.64020988],\n",
       "         [0.64006495],\n",
       "         [0.6400168 ],\n",
       "         [0.64110139]],\n",
       " \n",
       "        [[0.64224377],\n",
       "         [0.64209371],\n",
       "         [0.64204386],\n",
       "         [0.64316694]],\n",
       " \n",
       "        [[0.64414747],\n",
       "         [0.64399225],\n",
       "         [0.64394068],\n",
       "         [0.64510251]],\n",
       " \n",
       "        [[0.64592934],\n",
       "         [0.64576893],\n",
       "         [0.64571564],\n",
       "         [0.64691645]],\n",
       " \n",
       "        [[0.64759721],\n",
       "         [0.64743158],\n",
       "         [0.64737655],\n",
       "         [0.64861657]],\n",
       " \n",
       "        [[0.64915838],\n",
       "         [0.64898749],\n",
       "         [0.64893073],\n",
       "         [0.65021016]],\n",
       " \n",
       "        [[0.65061968],\n",
       "         [0.65044351],\n",
       "         [0.650385  ],\n",
       "         [0.65170404]],\n",
       " \n",
       "        [[0.65198748],\n",
       "         [0.65180602],\n",
       "         [0.65174575],\n",
       "         [0.65310459]],\n",
       " \n",
       "        [[0.65326778],\n",
       "         [0.653081  ],\n",
       "         [0.65301896],\n",
       "         [0.65441777]],\n",
       " \n",
       "        [[0.65446615],\n",
       "         [0.65427403],\n",
       "         [0.65421022],\n",
       "         [0.65564916]],\n",
       " \n",
       "        [[0.65558783],\n",
       "         [0.65539035],\n",
       "         [0.65532476],\n",
       "         [0.65680398]],\n",
       " \n",
       "        [[0.6566377 ],\n",
       "         [0.65643483],\n",
       "         [0.65636746],\n",
       "         [0.65788712]],\n",
       " \n",
       "        [[0.65762034],\n",
       "         [0.65741207],\n",
       "         [0.65734291],\n",
       "         [0.65890314]],\n",
       " \n",
       "        [[0.65854003],\n",
       "         [0.65832635],\n",
       "         [0.6582554 ],\n",
       "         [0.65985632]],\n",
       " \n",
       "        [[0.65940078],\n",
       "         [0.65918168],\n",
       "         [0.65910892],\n",
       "         [0.66075066]],\n",
       " \n",
       "        [[0.66020635],\n",
       "         [0.6599818 ],\n",
       "         [0.65990724],\n",
       "         [0.66158991]],\n",
       " \n",
       "        [[0.66096024],\n",
       "         [0.66073024],\n",
       "         [0.66065387],\n",
       "         [0.66237758]],\n",
       " \n",
       "        [[0.66166575],\n",
       "         [0.66143028],\n",
       "         [0.6613521 ],\n",
       "         [0.66311694]],\n",
       " \n",
       "        [[0.66232595],\n",
       "         [0.662085  ],\n",
       "         [0.66200501],\n",
       "         [0.66381108]],\n",
       " \n",
       "        [[0.66294372],\n",
       "         [0.66269729],\n",
       "         [0.66261547],\n",
       "         [0.66446286]],\n",
       " \n",
       "        [[0.66352177],\n",
       "         [0.66326983],\n",
       "         [0.6631862 ],\n",
       "         [0.66507498]],\n",
       " \n",
       "        [[0.66406261],\n",
       "         [0.66380517],\n",
       "         [0.66371971],\n",
       "         [0.66564996]],\n",
       " \n",
       "        [[0.66456861],\n",
       "         [0.66430566],\n",
       "         [0.66421837],\n",
       "         [0.66619017]],\n",
       " \n",
       "        [[0.66504199],\n",
       "         [0.66477351],\n",
       "         [0.66468439],\n",
       "         [0.66669781]],\n",
       " \n",
       "        [[0.66548482],\n",
       "         [0.66521081],\n",
       "         [0.66511986],\n",
       "         [0.66717495]],\n",
       " \n",
       "        [[0.66589904],\n",
       "         [0.6656195 ],\n",
       "         [0.66552671],\n",
       "         [0.66762354]],\n",
       " \n",
       "        [[0.66628648],\n",
       "         [0.66600138],\n",
       "         [0.66590676],\n",
       "         [0.66804538]],\n",
       " \n",
       "        [[0.66664882],\n",
       "         [0.66635818],\n",
       "         [0.66626172],\n",
       "         [0.66844217]],\n",
       " \n",
       "        [[0.66698767],\n",
       "         [0.66669147],\n",
       "         [0.66659317],\n",
       "         [0.66881552]],\n",
       " \n",
       "        [[0.66730451],\n",
       "         [0.66700276],\n",
       "         [0.66690262],\n",
       "         [0.6691669 ]],\n",
       " \n",
       "        [[0.66760076],\n",
       "         [0.66729344],\n",
       "         [0.66719146],\n",
       "         [0.66949771]],\n",
       " \n",
       "        [[0.66787771],\n",
       "         [0.66756482],\n",
       "         [0.66746099],\n",
       "         [0.66980926]],\n",
       " \n",
       "        [[0.66813658],\n",
       "         [0.66781812],\n",
       "         [0.66771245],\n",
       "         [0.67010277]],\n",
       " \n",
       "        [[0.66837854],\n",
       "         [0.66805451],\n",
       "         [0.66794699],\n",
       "         [0.67037939]],\n",
       " \n",
       "        [[0.66860465],\n",
       "         [0.66827504],\n",
       "         [0.66816568],\n",
       "         [0.67064019]],\n",
       " \n",
       "        [[0.66881592],\n",
       "         [0.66848073],\n",
       "         [0.66836952],\n",
       "         [0.67088618]],\n",
       " \n",
       "        [[0.66901329],\n",
       "         [0.66867252],\n",
       "         [0.66855946],\n",
       "         [0.6711183 ]],\n",
       " \n",
       "        [[0.66919765],\n",
       "         [0.66885129],\n",
       "         [0.66873638],\n",
       "         [0.67133742]],\n",
       " \n",
       "        [[0.66936982],\n",
       "         [0.66901787],\n",
       "         [0.66890112],\n",
       "         [0.67154437]],\n",
       " \n",
       "        [[0.66953058],\n",
       "         [0.66917303],\n",
       "         [0.66905443],\n",
       "         [0.67173994]],\n",
       " \n",
       "        [[0.66968064],\n",
       "         [0.66931751],\n",
       "         [0.66919706],\n",
       "         [0.67192483]],\n",
       " \n",
       "        [[0.6698207 ],\n",
       "         [0.66945197],\n",
       "         [0.66932967],\n",
       "         [0.67209972]],\n",
       " \n",
       "        [[0.66995138],\n",
       "         [0.66957706],\n",
       "         [0.66945291],\n",
       "         [0.67226526]],\n",
       " \n",
       "        [[0.67007328],\n",
       "         [0.66969337],\n",
       "         [0.66956736],\n",
       "         [0.67242204]],\n",
       " \n",
       "        [[0.67018696],\n",
       "         [0.66980145],\n",
       "         [0.6696736 ],\n",
       "         [0.6725706 ]],\n",
       " \n",
       "        [[0.67029294],\n",
       "         [0.66990183],\n",
       "         [0.66977213],\n",
       "         [0.67271149]],\n",
       " \n",
       "        [[0.67039171],\n",
       "         [0.66999501],\n",
       "         [0.66986345],\n",
       "         [0.67284517]],\n",
       " \n",
       "        [[0.67048372],\n",
       "         [0.67008143],\n",
       "         [0.66994802],\n",
       "         [0.67297211]],\n",
       " \n",
       "        [[0.67056942],\n",
       "         [0.67016152],\n",
       "         [0.67002627],\n",
       "         [0.67309274]],\n",
       " \n",
       "        [[0.67064919],\n",
       "         [0.6702357 ],\n",
       "         [0.67009859],\n",
       "         [0.67320745]],\n",
       " \n",
       "        [[0.67072341],\n",
       "         [0.67030432],\n",
       "         [0.67016537],\n",
       "         [0.67331663]],\n",
       " \n",
       "        [[0.67079244],\n",
       "         [0.67036776],\n",
       "         [0.67022695],\n",
       "         [0.67342063]],\n",
       " \n",
       "        [[0.67085661],\n",
       "         [0.67042632],\n",
       "         [0.67028367],\n",
       "         [0.67351976]],\n",
       " \n",
       "        [[0.67091622],\n",
       "         [0.67048034],\n",
       "         [0.67033584],\n",
       "         [0.67361435]],\n",
       " \n",
       "        [[0.67097156],\n",
       "         [0.67053008],\n",
       "         [0.67038373],\n",
       "         [0.67370468]],\n",
       " \n",
       "        [[0.67102291],\n",
       "         [0.67057584],\n",
       "         [0.67042764],\n",
       "         [0.67379103]],\n",
       " \n",
       "        [[0.67107052],\n",
       "         [0.67061785],\n",
       "         [0.6704678 ],\n",
       "         [0.67387363]],\n",
       " \n",
       "        [[0.67111462],\n",
       "         [0.67065636],\n",
       "         [0.67050446],\n",
       "         [0.67395274]],\n",
       " \n",
       "        [[0.67115544],\n",
       "         [0.67069159],\n",
       "         [0.67053784],\n",
       "         [0.67402857]],\n",
       " \n",
       "        [[0.67119319],\n",
       "         [0.67072374],\n",
       "         [0.67056815],\n",
       "         [0.67410134]],\n",
       " \n",
       "        [[0.67122806],\n",
       "         [0.67075302],\n",
       "         [0.67059558],\n",
       "         [0.67417123]],\n",
       " \n",
       "        [[0.67126024],\n",
       "         [0.6707796 ],\n",
       "         [0.67062032],\n",
       "         [0.67423842]],\n",
       " \n",
       "        [[0.67128989],\n",
       "         [0.67080366],\n",
       "         [0.67064253],\n",
       "         [0.6743031 ]],\n",
       " \n",
       "        [[0.67131717],\n",
       "         [0.67082536],\n",
       "         [0.67066238],\n",
       "         [0.67436542]],\n",
       " \n",
       "        [[0.67134225],\n",
       "         [0.67084484],\n",
       "         [0.67068002],\n",
       "         [0.67442553]],\n",
       " \n",
       "        [[0.67136525],\n",
       "         [0.67086225],\n",
       "         [0.67069559],\n",
       "         [0.67448356]],\n",
       " \n",
       "        [[0.67138631],\n",
       "         [0.67087772],\n",
       "         [0.67070922],\n",
       "         [0.67453966]],\n",
       " \n",
       "        [[0.67140555],\n",
       "         [0.67089138],\n",
       "         [0.67072103],\n",
       "         [0.67459395]],\n",
       " \n",
       "        [[0.67142308],\n",
       "         [0.67090333],\n",
       "         [0.67073113],\n",
       "         [0.67464653]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADAGRAD_BGD_LR(x,y,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db3aa062-84aa-4b8f-a41c-8421182f2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMS_BGD_LR(x, y, alpha=0.05, itirations=1000, beta=.9,gradient_norm=0.001, loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    \n",
    "    # Initialization and Data Preparation\n",
    "    m = x.shape[0]\n",
    "    loss = []\n",
    "    x = np.array(x)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    \n",
    "    # Add bias term (column of ones)\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    \n",
    "    # Initialize weights and the adaptive gradient accumulator with zeros\n",
    "    w = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    thetas = [] \n",
    "    adaptive = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    \n",
    "    for i in range(itirations):\n",
    "        # Hypothesis and Error calculation\n",
    "        hx = x @ w\n",
    "        e = hx - y\n",
    "        \n",
    "        # Calculate Cost Function (Mean Squared Error)\n",
    "        j = e.T @ e / (2 * m)\n",
    "        loss.append(j)\n",
    "        \n",
    "        # Calculate Gradient (G)\n",
    "        G = (x.T @ e) / m\n",
    "        \n",
    "        # Accumulate squared gradients for adaptive learning rate\n",
    "        adaptive =beta*adaptive+ (1-beta)*G**2\n",
    "        \n",
    "        # Compute the weight update step using the Adagrad formula\n",
    "        vt = (alpha / np.sqrt(adaptive + 1e-8)) * G\n",
    "        \n",
    "        # Save current weights before update\n",
    "        thetas.append(w.copy())\n",
    "        \n",
    "        # Check convergence based on Gradient Norm\n",
    "        if np.linalg.norm(G) <= (gradient_norm):\n",
    "            print(f''' Gradient Norm condition\n",
    "            stop in epoch number {i} \n",
    "            gradient norm = {np.linalg.norm(G)}''')\n",
    "            break\n",
    "            \n",
    "        # Check convergence based on relative Loss change\n",
    "        if i > 2 and np.absolute(loss[i-1] - loss[i]) < loss_condition:\n",
    "            print(f'''loss condition\n",
    "            stop in epoch number {i} \n",
    "            gradient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={loss[i]-loss[i-1]}\n",
    "            ''')\n",
    "            break\n",
    "        \n",
    "        # Update weights\n",
    "        w = w - vt\n",
    "        \n",
    "    # Final prediction and R2 score evaluation\n",
    "    hx = x @ w\n",
    "    print('you finished your iterations')\n",
    "    print(f'R2 score ={r2_score(y, hx)}')\n",
    "    \n",
    "    return np.array(w), hx, np.array(loss), np.array(thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a1cb4d-5a2a-4a6a-8836-22d5543beed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMS_BGD_LR(x, y, alpha=0.05, itirations=1000, beta=.9,gradient_norm=0.001, loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    \n",
    "    # Initialization and Data Preparation\n",
    "    m = x.shape[0]\n",
    "    loss = []\n",
    "    x = np.array(x)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    \n",
    "    # Add bias term (column of ones)\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    \n",
    "    # Initialize weights and the adaptive gradient accumulator with zeros\n",
    "    w = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    thetas = [] \n",
    "    adaptive = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    \n",
    "    for i in range(itirations):\n",
    "        # Hypothesis and Error calculation\n",
    "        hx = x @ w\n",
    "        e = hx - y\n",
    "        \n",
    "        # Calculate Cost Function (Mean Squared Error)\n",
    "        j = e.T @ e / (2 * m)\n",
    "        loss.append(j)\n",
    "        \n",
    "        # Calculate Gradient (G)\n",
    "        G = (x.T @ e) / m\n",
    "        \n",
    "        # Accumulate squared gradients for adaptive learning rate\n",
    "        adaptive =beta*adaptive+ (1-beta)*G**2\n",
    "        \n",
    "        # Compute the weight update step using the Adagrad formula\n",
    "        vt = (alpha / np.sqrt(adaptive + 1e-8)) * G\n",
    "        \n",
    "        # Save current weights before update\n",
    "        thetas.append(w.copy())\n",
    "        \n",
    "        # Check convergence based on Gradient Norm\n",
    "        if np.linalg.norm(G) <= (gradient_norm):\n",
    "            print(f''' Gradient Norm condition\n",
    "            stop in epoch number {i} \n",
    "            gradient norm = {np.linalg.norm(G)}''')\n",
    "            break\n",
    "            \n",
    "        # Check convergence based on relative Loss change\n",
    "        if i > 2 and np.absolute(loss[i-1] - loss[i]) < loss_condition:\n",
    "            print(f'''loss condition\n",
    "            stop in epoch number {i} \n",
    "            gradient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={loss[i]-loss[i-1]}\n",
    "            ''')\n",
    "            break\n",
    "        \n",
    "        # Update weights\n",
    "        w = w - vt\n",
    "        \n",
    "    # Final prediction and R2 score evaluation\n",
    "    hx = x @ w\n",
    "    print('you finished your iterations')\n",
    "    print(f'R2 score ={r2_score(y, hx)}')\n",
    "    \n",
    "    return np.array(w), hx, np.array(loss), np.array(thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24916bbe-a8f6-450e-bf04-daf5356b830d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80fa4f5e-861c-4234-9cea-673501bc58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAM_BGD_LR(x, y, alpha=0.05, itirations=1000,momentum_beta=.99, adaptive_beta=.9,gradient_norm=0.001, loss_condition=0.001):\n",
    "    from sklearn.metrics import r2_score\n",
    "    \n",
    "    # Initialization and Data Preparation\n",
    "    m = x.shape[0]\n",
    "    loss = []\n",
    "    x = np.array(x)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    \n",
    "    # Add bias term (column of ones)\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    \n",
    "    # Initialize weights and the adaptive gradient accumulator with zeros\n",
    "    w = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    thetas = [] \n",
    "    adaptive = np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    momentum= np.zeros(x.shape[1]).reshape(-1, 1)\n",
    "    \n",
    "    for i in range(itirations):\n",
    "        # Hypothesis and Error calculation\n",
    "        hx = x @ w\n",
    "        e = hx - y\n",
    "        \n",
    "        # Calculate Cost Function (Mean Squared Error)\n",
    "        j = e.T @ e / (2 * m)\n",
    "        loss.append(j)\n",
    "        \n",
    "        # Calculate Gradient (G)\n",
    "        G = (x.T @ e) / m\n",
    "        \n",
    "        \n",
    "        # Accumulate squared gradients for adaptive learning rate\n",
    "        adaptive =adaptive_beta*adaptive+ (1-adaptive_beta)*G**2\n",
    "        momentum=momentum_beta*momentum+(1-momentum_beta)*G\n",
    "        # Compute the weight update step using the Adagrad formula\n",
    "        vt = (alpha / np.sqrt(adaptive + 1e-8)) * momentum\n",
    "        \n",
    "        # Save current weights before update\n",
    "        thetas.append(w.copy())\n",
    "        \n",
    "        # Check convergence based on Gradient Norm\n",
    "        if np.linalg.norm(G) <= (gradient_norm):\n",
    "            print(f''' Gradient Norm condition\n",
    "            stop in epoch number {i} \n",
    "            gradient norm = {np.linalg.norm(G)}''')\n",
    "            break\n",
    "            \n",
    "        # Check convergence based on relative Loss change\n",
    "        if i > 2 and np.absolute(loss[i-1] - loss[i]) < loss_condition:\n",
    "            print(f'''loss condition\n",
    "            stop in epoch number {i} \n",
    "            gradient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={loss[i]-loss[i-1]}\n",
    "            ''')\n",
    "            break\n",
    "        \n",
    "        # Update weights\n",
    "        m_hat = momentum / (1 - momentum_beta**(i + 1))\n",
    "        v_hat = adaptive / (1 - adaptive_beta**(i + 1))\n",
    "        w = w - (alpha / (np.sqrt(v_hat) + 1e-8)) * m_hat\n",
    "        \n",
    "    # Final prediction and R2 score evaluation\n",
    "    hx = x @ w\n",
    "    print('you finished your iterations')\n",
    "    print(f'R2 score ={r2_score(y, hx)}')\n",
    "    \n",
    "    return np.array(w), hx, np.array(loss), np.array(thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639598f-2481-443b-ae16-bea5a633cd81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
