{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf53c8-4575-43ca-acfb-25ae3cedba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def NAG_MBGD_LR(x, y, alpha=0.05, batch_size=100, momentam_gama=0, epochs=10, gradient_norm=0.001, loss_condition=0.001):\n",
    "    # --- Data Preprocessing Phase ---\n",
    "    m = x.shape[0]\n",
    "    x = np.array(x)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    \n",
    "    # Shuffle data to ensure mini-batches are representative of the distribution\n",
    "    all_data = np.concatenate((x, y), axis=1)\n",
    "    np.random.shuffle(all_data)\n",
    "    \n",
    "    # Split back into features and target, adding the bias term (intercept)\n",
    "    x = all_data[:, :-1]\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    y = all_data[:, -1].reshape(-1, 1)\n",
    "    \n",
    "    # --- Initialization Phase ---\n",
    "    m = x.shape[0]\n",
    "    loss = []\n",
    "    w = np.zeros(x.shape[1]).reshape(-1, 1)      # Model weights\n",
    "    thetas = []                                  # History of weights for tracking\n",
    "    total_loss = []                              # Loss accumulated per epoch\n",
    "    momentam = np.zeros(x.shape[1]).reshape(-1, 1) # Velocity/Momentum vector\n",
    "    \n",
    "    # --- Training Loop ---\n",
    "    for i in range(epochs):\n",
    "        start = 0 \n",
    "        end = 0\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        # Iterating through data in mini-batches\n",
    "        while(start < m):\n",
    "            end = start + batch_size if (start + batch_size) < m else m\n",
    "            x_temp = x[start:end, :]\n",
    "            y_temp = y[start:end, :]\n",
    "            m_temp = x_temp.shape[0]\n",
    "            \n",
    "            # NAG Step 1: Look ahead by moving weights according to current momentum\n",
    "            w_temp = w - momentam_gama * momentam\n",
    "            \n",
    "            # Forward pass: Prediction and Error calculation\n",
    "            hx = x_temp @ w_temp\n",
    "            e = hx - y_temp\n",
    "            \n",
    "            # Calculate Mean Squared Error for the batch\n",
    "            j = e.T @ e / (2 * m_temp)\n",
    "            loss.append(j)\n",
    "            \n",
    "            # NAG Step 2: Compute gradient at the \"looked-ahead\" position\n",
    "            G = (x_temp.T @ e) / m_temp\n",
    "            \n",
    "            # NAG Step 3: Update velocity and weights\n",
    "            vt = alpha * G + momentam_gama * momentam\n",
    "            thetas.append(w.copy())\n",
    "            start = end\n",
    "            w = w - vt\n",
    "            momentam = vt # Update momentum for the next step\n",
    "            \n",
    "            total_epoch_loss += np.absolute(j)\n",
    "            \n",
    "        total_loss.append(total_epoch_loss)\n",
    "        \n",
    "        # --- Early Stopping Criteria ---\n",
    "        # 1. Stop if the gradient norm is sufficiently small\n",
    "        if np.linalg.norm(G) <= (gradient_norm):\n",
    "             print(f'Gradient Norm condition met. Stopped at epoch {i}')\n",
    "             break\n",
    "             \n",
    "        # 2. Stop if the change in total loss between epochs is negligible\n",
    "        if i > 2 and np.absolute(total_loss[i-1] - total_loss[i]) < loss_condition:\n",
    "            print(f'Loss convergence condition met. Stopped at epoch {i}')\n",
    "            break\n",
    "            \n",
    "    # --- Evaluation Phase (R2 Score calculation) ---\n",
    "    y_mean = np.mean(y)\n",
    "    ss_res, ss_tot = 0, 0\n",
    "    for s in range(0, m, batch_size):\n",
    "        e = min(s + batch_size, m)\n",
    "        y_p = x[s:e, :] @ w\n",
    "        ss_res += np.sum((y[s:e] - y_p)**2)\n",
    "        ss_tot += np.sum((y[s:e] - y_mean)**2)\n",
    "    \n",
    "    print(f'Final R2 Score = {1 - (ss_res / ss_tot)}') \n",
    "    print('Execution completed.')\n",
    "    \n",
    "    return np.array(w), np.array(loss), np.array(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e82efa-b0f7-4344-bd32-f40c03ab3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAG_MBGD_LR(x,y,  alpha=0.05,batch_size=100 , momentam_gama=0,epochs=10 ,  gradient_norm=0.001  ,  loss_condition=0.001):\n",
    "    m=x.shape[0]\n",
    "    x=np.array(x)\n",
    "    y=np.array(y).reshape(-1,1)\n",
    "    all_data=np.concatenate((x,y),axis=1)\n",
    "    np.random.shuffle(all_data)\n",
    "    x=all_data[:,:-1]\n",
    "    x = np.concatenate((np.ones((m, 1)), x), axis=1)\n",
    "    y=all_data[:,-1].reshape(-1,1)\n",
    "    m=x.shape[0]\n",
    "    loss=[]\n",
    "    w=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    w_temp=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    thetas=[] # return tensor \n",
    "    total_loss=[]\n",
    "    momentam=np.zeros(x.shape[1]).reshape(-1,1)\n",
    "    for i in range (epochs) :\n",
    "        start=0 \n",
    "        end= 0\n",
    "        total_epoch_loss=0\n",
    "        \n",
    "        while(start<m):\n",
    "            end=start+batch_size if (start+batch_size)<m else m\n",
    "            x_temp=x[start:end,:]\n",
    "            y_temp=y[start:end,:]\n",
    "            m_temp=x_temp.shape[0]\n",
    "            w_temp=w-momentam_gama*momentam\n",
    "            hx=x_temp@w_temp\n",
    "            e=hx-y_temp\n",
    "            j=e.T@e/(2*m_temp)\n",
    "            loss.append(j)\n",
    "            G=(x_temp.T@e)/m_temp\n",
    "            vt=alpha*G+momentam_gama*momentam\n",
    "            thetas.append(w.copy())\n",
    "            start=end\n",
    "            w=w-vt\n",
    "            momentam=vt\n",
    "            total_epoch_loss+=np.absolute(j)\n",
    "        total_loss.append(total_epoch_loss)\n",
    "        if np.linalg.norm(G)<=(gradient_norm):\n",
    "             print (f''' Gradient Norm condation\n",
    "             stop in epoch number {i} \n",
    "             gredient norm = {np.linalg.norm(G)}''')\n",
    "             break\n",
    "        if i>2 and np.absolute(total_loss[i-1]-total_loss[i])<loss_condition:\n",
    "            print (f'''loss condation\n",
    "            stop in epoch number {i} \n",
    "            gredient norm = {np.linalg.norm(G)}\n",
    "            loss[i]-loss[i-1]={total_loss[i-1]-total_loss[i]}\n",
    "            ''')\n",
    "            break\n",
    "    y_mean = np.mean(y)\n",
    "    ss_res, ss_tot = 0, 0\n",
    "    for s in range(0, m, batch_size):\n",
    "        e = min(s + batch_size, m)\n",
    "        y_p = x[s:e, :] @ w\n",
    "        ss_res += np.sum((y[s:e] - y_p)**2)\n",
    "        ss_tot += np.sum((y[s:e] - y_mean)**2)\n",
    "    \n",
    "    print(f'Final R2 Score = {1 - (ss_res / ss_tot)}') \n",
    "    print('you finshed your itirations ')\n",
    "    \n",
    "    return np.array (w) ,np.array(loss),np.array(thetas)\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
